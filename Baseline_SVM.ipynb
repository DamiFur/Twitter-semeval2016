{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fasttext\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import unidecode\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I couldn't import the data as directly downloaded from semeval because it gave me an \"unknown not utf-8 character\" so I imported it as a csv to an Excel and saved it again as train_utf.csv and it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.read_csv(\"semeval.abortion.train.csv\", sep=',', encoding=\"latin1\").fillna(method=\"ffill\")\n",
    "testdata = pd.read_csv(\"semeval_test_corrected.csv\", sep=',', encoding=\"latin1\").fillna(method=\"ffill\")\n",
    "testdata_wrong = pd.read_csv(\"semeval.abortion.test.csv\", sep=',', encoding=\"latin1\").fillna(method=\"ffill\")\n",
    "validationdata = pd.read_csv(\"semeval.abortion.validation.csv\", sep=',', encoding=\"latin1\").fillna(method=\"ffill\")\n",
    "\n",
    "is_against = traindata['Stance']=='AGAINST'\n",
    "is_favor = traindata['Stance']=='FAVOR'\n",
    "is_none = traindata['Stance']=='NONE'\n",
    "\n",
    "traindata_against = traindata[is_against]\n",
    "traindata_favor = traindata[is_favor]\n",
    "traindata_none = traindata[is_none]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsampling to equilibrate the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 43\n",
    "# traindata_balanced = pd.concat([traindata_against.sample(n=99, random_state=seed), traindata_none.sample(n=99, random_state=seed), traindata_favor.sample(frac=1.0, random_state=seed)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TweetTokenizer basically unterstands arrows, smiley faces and weird punctuation\n",
    "tokenizer = TweetTokenizer(preserve_case=True, reduce_len=True, strip_handles=False)\n",
    "\n",
    "\n",
    "def my_preprocess(text, keep_hashtags=True):\n",
    "    toks = tokenizer.tokenize(text)\n",
    "\n",
    "    ret = []\n",
    "    for tok in toks:\n",
    "#         if tok[:4] == \"#sem\":\n",
    "#             continue\n",
    "        if tok[0] == \"#\" and not keep_hashtags:\n",
    "            continue\n",
    "        if tok[:4] == \"http\":\n",
    "            continue\n",
    "        if tok[0] == \"@\":\n",
    "            continue\n",
    "        # removing numbers\n",
    "#         if tok.isnumeric():\n",
    "#             continue\n",
    "        ret.append(tok)\n",
    "    return \" \".join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train\n",
    "traindata['Text'] = traindata['Tweet'].apply(lambda x: my_preprocess(x))\n",
    "\n",
    "# Shifts the order on the original tweet list\n",
    "sample_train = traindata.sample(frac=1.0)\n",
    "\n",
    "text_train, label_train = sample_train['Text'], sample_train['Stance']\n",
    "\n",
    "\n",
    "# # ## Train Balanced (with sub-sampling)\n",
    "# traindata_balanced['Text'] = traindata_balanced['Tweet'].apply(lambda x: my_preprocess(x))\n",
    "\n",
    "# # Shifts the order on the original tweet list\n",
    "# sample_train_balanced = traindata_balanced.sample(frac=1.0)\n",
    "\n",
    "# text_train_balanced, label_train_balanced = sample_train_balanced['Text'], sample_train_balanced['Stance']\n",
    "\n",
    "\n",
    "## Validation\n",
    "validationdata['Text'] = validationdata['Tweet'].apply(lambda x: my_preprocess(x))\n",
    "\n",
    "# Shifts the order on the original tweet list\n",
    "sample_valid = validationdata.sample(frac=1.0)\n",
    "\n",
    "text_valid, label_valid = sample_valid['Text'], sample_valid['Stance']\n",
    "\n",
    "\n",
    "\n",
    "## Test\n",
    "testdata['Text'] = testdata['Tweet'].apply(lambda x: my_preprocess(x))\n",
    "\n",
    "sample_test = testdata.sample(frac=1.0)\n",
    "\n",
    "text_test, label_test = sample_test['Text'], sample_test['MyLabeling']\n",
    "\n",
    "\n",
    "## Test with original badly anotated dataset\n",
    "\n",
    "testdata_wrong['Text'] = testdata_wrong['Tweet'].apply(lambda x: my_preprocess(x))\n",
    "\n",
    "sample_test_wrong = testdata_wrong.sample(frac=1.0)\n",
    "\n",
    "text_test_wrong, label_test_wrong = sample_test_wrong['Text'], sample_test_wrong['Stance']\n",
    "\n",
    "# print(list(zip(text_test, label_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add or remove a '_balanced' after the text_train and label_train\n",
    "texttrain = text_train\n",
    "labeltrain = label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def camel_case_split(identifier):\n",
    "    if identifier == \"#SemST\":\n",
    "        return []\n",
    "    if identifier[0] == '#':\n",
    "        matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier[1:])\n",
    "        return [m.group(0) for m in matches]\n",
    "    else:\n",
    "        return [identifier]\n",
    "\n",
    "# add '_wrong' or not after the variables\n",
    "texttest, labeltest = [\" \".join([\" \".join(camel_case_split(word)) for word in text.split(\" \")]) for text in text_test_wrong], label_test_wrong\n",
    "textvalid, labelvalid = [\" \".join([\" \".join(camel_case_split(word)) for word in text.split(\" \")]) for text in text_valid], label_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation of tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    binary=True, min_df=0.0075, max_df=0.75, ngram_range=(1, 5),\n",
    "    #stop_words=stopwords.words('spanish')\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform([*texttrain, *textvalid, *texttest])\n",
    "# VEC_train = vectorizer.fit_transform([*texttrain])\n",
    "# VEC_valid = vectorizer.transform([*textvalid])\n",
    "# VEC_test = vectorizer.transform([*texttest])\n",
    "\n",
    "# print(X.shape)\n",
    "\n",
    "VEC_train = X[:len(texttrain)]\n",
    "VEC_valid = X[len(texttrain):len(texttrain) + len(textvalid)]\n",
    "VEC_test = X[len(texttrain) + len(textvalid):len(texttrain) + len(textvalid) + len(texttest)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(522, 426)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VEC_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vectors = fasttext.load_model(\"fasttext-embeddings-abortion.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'polynomial' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-7c0f086a52f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msvc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'polynomial'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scale'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVEC_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeltrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_sparse_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mkernel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_kernels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mlibsvm_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'polynomial' is not in list"
     ]
    }
   ],
   "source": [
    "svc = svm.SVC(probability=True, kernel='polynomial', gamma='scale')\n",
    "svc.fit(VEC_train, labeltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9846743295019157"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(VEC_train, labeltrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6071428571428571"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "svc.score(VEC_test, labeltest)\n",
    "\n",
    "predictions = []\n",
    "for vec_test, label, tweet in zip(VEC_test, labeltest, texttest):\n",
    "    predictions.append(svc.predict(vec_test))\n",
    "    \n",
    "f1_score(labeltest, predictions, average=None, labels=[\"AGAINST\", \"FAVOR\", \"NONE\"])\n",
    "accuracy_score(labeltest, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for vec_test, label, tweet in zip(VEC_test, labeltest, texttest):\n",
    "    predictions.append(svc.predict(vec_test))\n",
    "#     print(label)\n",
    "#     print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65217391, 0.52554745, 0.61139896])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(labeltest, predictions, average=None, labels=[\"AGAINST\", \"FAVOR\", \"NONE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6071428571428571"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labeltest, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_prob(probs):\n",
    "    highest = 0\n",
    "    for i in range(3):\n",
    "        if probs[i] > highest:\n",
    "            highest = probs[i]\n",
    "    return highest\n",
    "\n",
    "def count_above_threshold(probs, threshold):\n",
    "    count = 0\n",
    "    for i in range(3):\n",
    "        if probs[i] > threshold:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_probs = []\n",
    "tweets_selected = []\n",
    "label_valid_selection = []\n",
    "predictions = []\n",
    "for vec_test, label, tweet in zip(VEC_test, labeltest, texttest):\n",
    "    predictions_probs.append(svc.predict_proba(vec_test)[0])\n",
    "    tweets_selected.append(tweet)\n",
    "    label_valid_selection.append(label)\n",
    "    predictions.append(svc.predict(vec_test)[0])\n",
    "tweet_truth_prob_pred = zip(tweets_selected, label_valid_selection, predictions_probs, predictions)\n",
    "tweet_truth_prob_pred = sorted(tweet_truth_prob_pred, key=lambda x: -highest_prob(x[2]) if(count_above_threshold(x[2], 0.25) == 1) else 1)\n",
    "\n",
    "predictions_sorted = []\n",
    "truth_sorted = []\n",
    "for tweet, truth, probs, pred in tweet_truth_prob_pred:\n",
    "    predictions_sorted.append(pred)\n",
    "    truth_sorted.append(truth)\n",
    "\n",
    "f1_score(truth_sorted[:10], predictions_sorted[:10], average=\"micro\", labels=[\"AGAINST\", \"FAVOR\", \"NONE\"])\n",
    "\n",
    "#     if highest_prob(probs[0]) > 0.8 and count_above_threshold(probs[0], 0.3) == 1:\n",
    "#         print(tweet)\n",
    "#         print(svc.predict(vec_test)[0])\n",
    "#         print(label)\n",
    "#         print(svc.predict_proba(vec_test))\n",
    "#     print(label)\n",
    "#     print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial value\n",
    "enhanced_text_train = list(texttrain)\n",
    "enhanced_label_train = list(labeltrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_number = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Aren ’ t politicians meant to be intelligent ? Politicians debate changing ' pregnant women ' to ' pregnant people '\",\n",
       "  array([0.03379076, 0.94322589, 0.02298335]),\n",
       "  'FAVOR'),\n",
       " ('I support Pro Life . Abortion is the murder of an innocent human',\n",
       "  array([0.92886476, 0.06179444, 0.0093408 ]),\n",
       "  'AGAINST'),\n",
       " ('Really ? False ? Are you genuinely unaware that abortion is the murder of a baby ?',\n",
       "  array([0.91668567, 0.04754896, 0.03576537]),\n",
       "  'AGAINST'),\n",
       " ('ATP : Abortion : GOP Virginia Candidate – Ankle Bracelets for Pregnant Women abortion Virginia prochoice',\n",
       "  array([0.06152358, 0.91428169, 0.02419473]),\n",
       "  'FAVOR'),\n",
       " ('Wow , Virginia is about to enter the 21st century . Hold on to your tricorner hats .',\n",
       "  array([0.06933862, 0.02262946, 0.90803192]),\n",
       "  'NONE'),\n",
       " ('This is a bad judge .',\n",
       "  array([0.05758508, 0.03451112, 0.9079038 ]),\n",
       "  'NONE'),\n",
       " ('This is Democrats', array([0.05758508, 0.03451112, 0.9079038 ]), 'NONE'),\n",
       " ('This is fucking disgusting .',\n",
       "  array([0.05758508, 0.03451112, 0.9079038 ]),\n",
       "  'NONE'),\n",
       " ('This is disgusting ...',\n",
       "  array([0.05758508, 0.03451112, 0.9079038 ]),\n",
       "  'NONE'),\n",
       " ('Texas House tanks Pro-Life priorities , ignores grassroots supporters and the cries of the unborn',\n",
       "  array([0.90607081, 0.05875781, 0.03517139]),\n",
       "  'AGAINST'),\n",
       " ('This 🙌 🏽', array([0.06172358, 0.03300688, 0.90526954]), 'NONE'),\n",
       " ('My hero ! !', array([0.06418426, 0.03366122, 0.90215452]), 'NONE'),\n",
       " ('My pleasure Lori', array([0.06418426, 0.03366122, 0.90215452]), 'NONE'),\n",
       " ('prays for the Unborn Follow these Pro Life Patriots 👼 👼 👼 👼 👼 👼 👼 👼 👼 👼 👼 👼 👼',\n",
       "  array([0.90166161, 0.05785565, 0.04048274]),\n",
       "  'AGAINST'),\n",
       " ('Stop The Black Genocide Choose Life Abortion Is Murder',\n",
       "  array([0.89728975, 0.07546081, 0.02724944]),\n",
       "  'AGAINST'),\n",
       " ('is swallowing cum cannibalism ?',\n",
       "  array([0.07107411, 0.03296002, 0.89596587]),\n",
       "  'NONE'),\n",
       " ('Homosexuality is a sin',\n",
       "  array([0.07107411, 0.03296002, 0.89596587]),\n",
       "  'NONE'),\n",
       " ('Guess I ’ m not watching 🙄',\n",
       "  array([0.05585365, 0.04895778, 0.89518857]),\n",
       "  'NONE'),\n",
       " ('Not a chance .', array([0.05585365, 0.04895778, 0.89518857]), 'NONE'),\n",
       " (\"Trump's Anti-Abortion Policies Have Created A Nightmare For Women In Nepal via\",\n",
       "  array([0.07642397, 0.89501453, 0.0285615 ]),\n",
       "  'FAVOR')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_number = 10\n",
    "\n",
    "f = open(\"random_partitions_of_unlabeled_corpus/abortion_unlabeled_sample_{}.csv\".format(corpus_number), 'r')\n",
    "predictions = []\n",
    "predictions_probs = []\n",
    "tweets = []\n",
    "for idx, line in enumerate(f):\n",
    "    tweet = line.replace('\\n','')\n",
    "    tweets.append(tweet)\n",
    "    Y = vectorizer.transform([tweet])\n",
    "    predictions.append(svc.predict(Y)[0])\n",
    "    predictions_probs.append(svc.predict_proba(Y)[0])\n",
    "\n",
    "tweet_prob_pred = zip(tweets, predictions_probs, predictions)\n",
    "tweet_prob_pred = sorted(tweet_prob_pred, key=lambda x: -highest_prob(x[1]) if(count_above_threshold(x[1], 0.25) == 1) else 1)\n",
    "\n",
    "#     prediction = svc.predict(Y)[0]\n",
    "#     tweets.append(tweet)\n",
    "#     predictions.append(prediction)\n",
    "    \n",
    "# tweet_pred = zip(tweets, predictions)\n",
    "# tweet_pred = sorted(tweet_pred, key=lambda p: -p[1][1][0])\n",
    "\n",
    "tweet_prob_pred[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance the predicted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_count = 0\n",
    "fv_count = 0\n",
    "none_count = 0\n",
    "pred_limit = 20\n",
    "against_limit = 50 * 0.5\n",
    "favor_limit = 26 * 0.5\n",
    "none_limit = 24 * 0.5\n",
    "enhanced_text = []\n",
    "enhanced_label = []\n",
    "for tweet, prob, pred in tweet_prob_pred:\n",
    "    if pred == \"AGAINST\" and ag_count < against_limit:\n",
    "#         train_ft.append(\"{} - {} - {}\".format(tweet, pred, prob))\n",
    "        enhanced_text.append(tweet)\n",
    "        enhanced_label.append(pred)\n",
    "        ag_count += 1\n",
    "    if pred == \"FAVOR\" and fv_count < favor_limit:\n",
    "#         train_ft.append(\"{} - {} - {}\".format(tweet, pred, prob))\n",
    "        enhanced_text.append(tweet)\n",
    "        enhanced_label.append(pred)\n",
    "        fv_count += 1\n",
    "    if pred == \"NONE\" and none_count < none_limit:\n",
    "#         train_ft.append(\"{} - {} - {}\".format(tweet, pred, prob))\n",
    "        enhanced_text.append(tweet)\n",
    "        enhanced_label.append(pred)\n",
    "        none_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "922"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_text_train = enhanced_text_train + enhanced_text\n",
    "enhanced_label_train = enhanced_label_train + enhanced_label\n",
    "len(enhanced_text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    binary=True, min_df=0.0075, max_df=0.75, ngram_range=(1, 5),\n",
    "    #stop_words=stopwords.words('spanish')\n",
    ")\n",
    "\n",
    "X_enhanced = vectorizer.fit_transform([*enhanced_text_train, *textvalid, *texttest])\n",
    "# VEC_train = vectorizer.fit_transform([*texttrain])\n",
    "# VEC_valid = vectorizer.transform([*textvalid])\n",
    "# VEC_test = vectorizer.transform([*texttest])\n",
    "\n",
    "# print(X.shape)\n",
    "\n",
    "VEC_train_enhanced = X_enhanced[:len(enhanced_text_train)]\n",
    "VEC_valid_enhanced = X_enhanced[len(enhanced_text_train):len(enhanced_text_train) + len(textvalid)]\n",
    "VEC_test_enhanced = X_enhanced[len(enhanced_text_train) + len(textvalid):len(enhanced_text_train) + len(textvalid) + len(texttest)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9620390455531453"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_enhanced = svc = svm.SVC(probability=True, kernel='rbf')#, gamma='scale')\n",
    "svc_enhanced.fit(VEC_train_enhanced, enhanced_label_train)\n",
    "svc_enhanced.score(VEC_train_enhanced, enhanced_label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6535714285714286"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_enhanced.score(VEC_test_enhanced, labeltest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_enhanced.score(VEC_test_enhanced, labeltest)\n",
    "predictions = []\n",
    "labels = []\n",
    "for vec_test, label in zip(VEC_test_enhanced, labeltest):\n",
    "    predictions.append(svc_enhanced.predict(vec_test)[0])\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AGAINST', 'NONE'),\n",
       " ('NONE', 'NONE'),\n",
       " ('FAVOR', 'NONE'),\n",
       " ('NONE', 'FAVOR'),\n",
       " ('NONE', 'NONE'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'FAVOR'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'FAVOR'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('NONE', 'FAVOR'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('FAVOR', 'NONE'),\n",
       " ('NONE', 'NONE'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('NONE', 'NONE'),\n",
       " ('NONE', 'NONE'),\n",
       " ('NONE', 'NONE'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('NONE', 'NONE'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('NONE', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'FAVOR'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('NONE', 'FAVOR'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('FAVOR', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'FAVOR'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'AGAINST'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('NONE', 'NONE'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('NONE', 'NONE'),\n",
       " ('NONE', 'NONE'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'AGAINST'),\n",
       " ('NONE', 'AGAINST'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('FAVOR', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('FAVOR', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('NONE', 'NONE'),\n",
       " ('NONE', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('FAVOR', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('NONE', 'NONE'),\n",
       " ('FAVOR', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('FAVOR', 'NONE'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'FAVOR'),\n",
       " ('NONE', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('NONE', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'AGAINST'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'FAVOR'),\n",
       " ('NONE', 'FAVOR'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('FAVOR', 'AGAINST'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('FAVOR', 'NONE'),\n",
       " ('FAVOR', 'NONE'),\n",
       " ('NONE', 'AGAINST'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('NONE', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('FAVOR', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('NONE', 'NONE'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('NONE', 'NONE'),\n",
       " ('NONE', 'NONE'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('NONE', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('NONE', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'FAVOR'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('FAVOR', 'FAVOR'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('FAVOR', 'AGAINST'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('AGAINST', 'AGAINST'),\n",
       " ('NONE', 'AGAINST'),\n",
       " ('NONE', 'NONE'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('AGAINST', 'FAVOR'),\n",
       " ('FAVOR', 'AGAINST'),\n",
       " ('AGAINST', 'NONE'),\n",
       " ('NONE', 'AGAINST')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6953125 , 0.55714286, 0.67073171])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(labels, predictions, average=None, labels=[\"AGAINST\", \"FAVOR\", \"NONE\"])\n",
    "accuracy_score(labeltest, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6535714285714286"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labeltest, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_number += 1\n",
    "corpus_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\" Women Denied Abortion May Face Long-Lasting Health Problems \" - Study',\n",
       "  array([0.00198839, 0.99307007, 0.00494154]),\n",
       "  'FAVOR'),\n",
       " (\"Please support women's rights to reproductive health and access to services .\",\n",
       "  array([0.00294184, 0.98778602, 0.00927214]),\n",
       "  'FAVOR'),\n",
       " ('Factors influencing decision-making power regarding reproductive health and rights among married women in Mettu rural district , south-west , Ethiopia :',\n",
       "  array([0.00263875, 0.98769769, 0.00966356]),\n",
       "  'FAVOR'),\n",
       " ('Nairobi summit on ICPD 25 to push for strong commitments to sexual and reproductive health and rights -',\n",
       "  array([0.00401114, 0.98442427, 0.01156459]),\n",
       "  'FAVOR'),\n",
       " ('Who Speaks For The Child Abortion Is Murder Heartbeat Is Life',\n",
       "  array([0.97631116, 0.01566921, 0.00801963]),\n",
       "  'AGAINST'),\n",
       " ('Pray The Rosary . First Friday . Ave Maria . Blessed Virgin Mary . Our Lady . Hail Holy Queen . Sacred Heart Of Jesus . Catholic . Ave Maria Gratia Plena . Catholics . Pray To End Abortion . Pro Life . Catholic Church . Catholic Twitter . Catholic Faith . Catholic Saints . Holy Mother Of God . .',\n",
       "  array([0.97606366, 0.01539469, 0.00854165]),\n",
       "  'AGAINST'),\n",
       " ('Comprehensive sexual and reproductive health services are critical to the lives of women and girls ! a grantee partner of made this message loud and clear during their mock tribunals on abortion rights last week !',\n",
       "  array([0.01276878, 0.97605309, 0.01117812]),\n",
       "  'FAVOR'),\n",
       " ('O Virgin of Guadalupe , ⚘ patroness of unborn children , we implore your intercession ... pray for all the babies in danger of being aborted ! ! Amen Pro Life Save The Tiny Humans',\n",
       "  array([0.97526386, 0.01696302, 0.00777312]),\n",
       "  'AGAINST'),\n",
       " (\"Less than half of US evangelicals identify as ' pro-life , ' new poll suggests abortion Pro Life\",\n",
       "  array([0.97524917, 0.0188464 , 0.00590443]),\n",
       "  'AGAINST'),\n",
       " ('\" Presley was one of several abortion survivors who spoke at a pro-life panel at the 2019 Values Voter Summit last month . \" believe All Survivors ?',\n",
       "  array([0.97468058, 0.01361183, 0.01170759]),\n",
       "  'AGAINST'),\n",
       " ('Pro Life Democrat Democrats Are Destroying America Abortion Is Murder PPC',\n",
       "  array([0.97284946, 0.02070982, 0.00644072]),\n",
       "  'AGAINST'),\n",
       " ('Let this sink in ...', array([0.01167361, 0.01648485, 0.97184154]), 'NONE'),\n",
       " ('The “ Make Families Great Again Conference ... demonstrates the insidious connections between authoritarian governments , far-right nationalists , and anti-choice movements who conspire to strip women and gender diverse people of their rights . \"',\n",
       "  array([0.01671217, 0.96986747, 0.01342037]),\n",
       "  'FAVOR'),\n",
       " ('The whole thread is fascinating to watch ...',\n",
       "  array([0.02352064, 0.00663398, 0.96984538]),\n",
       "  'NONE'),\n",
       " ('Biased Judge Ban Key Pro-Life Witness From Planned Parenthood Aborted Baby Parts Trial |',\n",
       "  array([0.96918627, 0.0197574 , 0.01105633]),\n",
       "  'AGAINST'),\n",
       " ('This ride 🔥 🔥 🔥 🇺 🇸 😎 👍 R / T FA 🇺 🇸',\n",
       "  array([0.01420146, 0.01744379, 0.96835475]),\n",
       "  'NONE'),\n",
       " ('This ! 👇', array([0.01420146, 0.01744379, 0.96835475]), 'NONE'),\n",
       " ('Bro . Chill . THE FUCK . out .',\n",
       "  array([0.0267334 , 0.00657299, 0.96669361]),\n",
       "  'NONE'),\n",
       " ('My pleasure , Annie', array([0.01605731, 0.01753601, 0.96640669]), 'NONE'),\n",
       " ('Decent work for women calls for sexual and reproductive rights . + in the opinions section --->',\n",
       "  array([0.01272032, 0.96503975, 0.02223993]),\n",
       "  'FAVOR')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"random_partitions_of_unlabeled_corpus/abortion_unlabeled_sample_{}.csv\".format(corpus_number), 'r')\n",
    "predictions = []\n",
    "predictions_probs = []\n",
    "tweets = []\n",
    "for idx, line in enumerate(f):\n",
    "    tweet = line.replace('\\n','')\n",
    "    tweets.append(tweet)\n",
    "    Y = vectorizer.transform([tweet])\n",
    "    predictions.append(svc_enhanced.predict(Y)[0])\n",
    "    predictions_probs.append(svc_enhanced.predict_proba(Y)[0])\n",
    "\n",
    "tweet_prob_pred = zip(tweets, predictions_probs, predictions)\n",
    "tweet_prob_pred = sorted(tweet_prob_pred, key=lambda x: -highest_prob(x[1]) if(count_above_threshold(x[1], 0.25) == 1) else 1)\n",
    "\n",
    "tweet_prob_pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparametros a testear:\n",
    "#     Top K parameters: fijo en 200\n",
    "#     Kernel functions\n",
    "#     C\n",
    "#     3 iteraciones de bootstrap. Se puede testear hasta 10 en dev\n",
    "#     Agregar según la distribución del training o segun los primeros q aparezcan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alltogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_prob(probs):\n",
    "    highest = 0\n",
    "    for i in range(3):\n",
    "        if probs[i] > highest:\n",
    "            highest = probs[i]\n",
    "    return highest\n",
    "\n",
    "def count_above_threshold(probs, threshold):\n",
    "    count = 0\n",
    "    for i in range(3):\n",
    "        if probs[i] > threshold:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial corpus: 1\n",
      "Metrics on test on initial classifier\n",
      "[0.75718016 0.33333333 0.43243243]\n",
      "macro: 0.5076486408079098\n",
      "micro: 0.6428571428571429\n",
      "precision\n",
      "[0.74742268 0.55       0.36363636]\n",
      "macro: 0.553686348016245\n",
      "micro: 0.6428571428571429\n",
      "recall\n",
      "[0.76719577 0.23913043 0.53333333]\n",
      "macro: 0.5132198451039031\n",
      "micro: 0.6428571428571429\n",
      "accuracy: 0.6428571428571429\n",
      "Training set original size: 522\n",
      "Training set new size: 698\n",
      "Training set new size: 898\n",
      "Training set new size: 1098\n",
      "Training set new size: 1298\n",
      "Training set new size: 1498\n",
      "Training set original size: 522\n",
      "Training set new size: 695\n",
      "Training set new size: 895\n",
      "Training set new size: 1095\n",
      "Training set new size: 1295\n",
      "Training set new size: 1495\n",
      "Training set original size: 522\n",
      "Training set new size: 699\n",
      "Training set new size: 899\n",
      "Training set new size: 1099\n",
      "Training set new size: 1299\n",
      "Training set new size: 1499\n",
      "Training set original size: 522\n",
      "Training set new size: 696\n",
      "Training set new size: 896\n",
      "Training set new size: 1096\n",
      "Training set new size: 1296\n",
      "Training set new size: 1496\n",
      "Training set original size: 522\n",
      "Training set new size: 702\n",
      "Training set new size: 902\n",
      "Training set new size: 1102\n",
      "Training set new size: 1302\n",
      "Training set new size: 1502\n",
      "0.5533197109909469,0.610855917099613,0.6492857142857142,0.7545257858859442,0.4671860483132818,0.6492857142857142,0.7806030478842606,0.5405414067914067,0.6605722273378337,0.5616778904355367,0.73015873015873,0.4130434782608695,0.5716011042097998,0.564771106510237\n",
      "0.5715977216045198,0.6417949792544666,0.6385714285714286,0.7341534116707249,0.5494365468382083,0.6385714285714286,0.8144175742174111,0.5130335099626896,0.6637255420900503,0.5599783440954298,0.6687830687830688,0.5956521739130436,0.6322176213480561,0.6066635994172226\n",
      "0.5779466578020276,0.6473004330530797,0.6392857142857143,0.731207504018531,0.5633933620876285,0.6392857142857143,0.8244650188104543,0.5073696393762184,0.6659173290933363,0.5632302857950141,0.6571428571428571,0.6347826086956523,0.6459627329192547,0.6202714515757994\n",
      "0.5667258649942667,0.632447467135283,0.6257142857142857,0.7178084898801598,0.5470864443904062,0.6257142857142857,0.8242432976380345,0.47901888191218955,0.6516310897751121,0.5519354934542365,0.635978835978836,0.6391304347826087,0.6375546353807223,0.6146660532167779\n",
      "0.5611231785641767,0.6233543702203683,0.6207142857142858,0.7136481496553754,0.5330605907853614,0.6207142857142858,0.8188022644474504,0.47052177166730064,0.6446620180573754,0.5473337134141492,0.6328042328042327,0.6173913043478261,0.6250977685760295,0.607842956828464\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kernel = 'rbf'\n",
    "\n",
    "random.seed(43)\n",
    "\n",
    "corpus_number = random.randrange(0,20,1)\n",
    "\n",
    "print(\"initial corpus: {}\".format(corpus_number))\n",
    "\n",
    "pred_limit = 200\n",
    "against_limit = 50 * 2\n",
    "favor_limit = 26 * 2\n",
    "none_limit = 24 * 2\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    binary=True, min_df=0.0075, max_df=0.75, ngram_range=(1, 5),\n",
    "    #stop_words=stopwords.words('spanish')\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform([*texttrain, *textvalid, *texttest])\n",
    "\n",
    "VEC_train = X[:len(texttrain)]\n",
    "VEC_valid = X[len(texttrain):len(texttrain) + len(textvalid)]\n",
    "VEC_test = X[len(texttrain) + len(textvalid):len(texttrain) + len(textvalid) + len(texttest)]\n",
    "\n",
    "\n",
    "svc = svm.SVC(probability=True, kernel=kernel, gamma='scale')#, class_weight='balanced')\n",
    "svc.fit(VEC_train, labeltrain)\n",
    "\n",
    "# svc.score(VEC_train, labeltrain)\n",
    "# svc.score(VEC_test, labeltest)\n",
    "\n",
    "predictions = []\n",
    "for vec_test, label, tweet in zip(VEC_test, labeltest, texttest):\n",
    "    predictions.append(svc.predict(vec_test))\n",
    "\n",
    "print(\"Metrics on test on initial classifier\")\n",
    "print(f1_score(labeltest, predictions, average=None, labels=[\"AGAINST\", \"FAVOR\", \"NONE\"]))\n",
    "print(\"macro: {}\".format(f1_score(labeltest, predictions, average=\"macro\", labels=[\"AGAINST\", \"FAVOR\", \"NONE\"])))\n",
    "print(\"micro: {}\".format(f1_score(labeltest, predictions, average=\"micro\", labels=[\"AGAINST\", \"FAVOR\", \"NONE\"])))\n",
    "print(\"precision\")\n",
    "print(precision_score(labeltest, predictions, average=None, labels=[\"AGAINST\", \"FAVOR\", \"NONE\"]))\n",
    "print(\"macro: {}\".format(precision_score(labeltest, predictions, average=\"macro\", labels=[\"AGAINST\", \"FAVOR\", \"NONE\"])))\n",
    "print(\"micro: {}\".format(precision_score(labeltest, predictions, average=\"micro\", labels=[\"AGAINST\", \"FAVOR\", \"NONE\"])))\n",
    "print(\"recall\")\n",
    "print(recall_score(labeltest, predictions, average=None, labels=[\"AGAINST\", \"FAVOR\", \"NONE\"]))\n",
    "print(\"macro: {}\".format(recall_score(labeltest, predictions, average=\"macro\", labels=[\"AGAINST\", \"FAVOR\", \"NONE\"])))\n",
    "print(\"micro: {}\".format(recall_score(labeltest, predictions, average=\"micro\", labels=[\"AGAINST\", \"FAVOR\", \"NONE\"])))\n",
    "\n",
    "print(\"accuracy: {}\".format(accuracy_score(labeltest, predictions)))\n",
    "\n",
    "# Initial value\n",
    "enhanced_text_train = list(texttrain)\n",
    "enhanced_label_train = list(labeltrain)\n",
    "\n",
    "f1_av = [0] * 5\n",
    "f1_against = [0] * 5\n",
    "f1_favor = [0] * 5\n",
    "macro = [0] * 5\n",
    "micro = [0] * 5\n",
    "prec_against = [0] * 5\n",
    "prec_favor = [0] * 5\n",
    "prec_av = [0] * 5\n",
    "prec_macro = [0] * 5\n",
    "rec_against = [0] * 5\n",
    "rec_favor = [0] * 5\n",
    "rec_av = [0] * 5\n",
    "rec_macro = [0] * 5\n",
    "acc = [0] * 5\n",
    "for j in range(5):\n",
    "    random.seed(43 + j)\n",
    "    corpus_number = random.randrange(0,20,1)\n",
    "    f = open(\"random_partitions_of_unlabeled_corpus/abortion_unlabeled_sample_{}.csv\".format(corpus_number), 'r')\n",
    "    predictions = []\n",
    "    predictions_probs = []\n",
    "    tweets = []\n",
    "    for idx, line in enumerate(f):\n",
    "        tweet = line.replace('\\n','')\n",
    "        tweets.append(tweet)\n",
    "        Y = vectorizer.transform([tweet])\n",
    "        predictions.append(svc.predict(Y)[0])\n",
    "        predictions_probs.append(svc.predict_proba(Y)[0])\n",
    "\n",
    "    tweet_prob_pred = zip(tweets, predictions_probs, predictions)\n",
    "    tweet_prob_pred = sorted(tweet_prob_pred, key=lambda x: -highest_prob(x[1]) if(count_above_threshold(x[1], 0.25) == 1) else 1)\n",
    "\n",
    "    # Initial value\n",
    "    enhanced_text_train = list(texttrain)\n",
    "    enhanced_label_train = list(labeltrain)\n",
    "    print(\"Training set original size: {}\".format(len(enhanced_text_train)))\n",
    "\n",
    "\n",
    "    for i in range(5):\n",
    "\n",
    "        # Balance the predicted data\n",
    "        ag_count = 0\n",
    "        fv_count = 0\n",
    "        none_count = 0\n",
    "        enhanced_text = []\n",
    "        enhanced_label = []\n",
    "        for tweet, prob, pred in tweet_prob_pred:\n",
    "#             enhanced_text.append(tweet)\n",
    "#             enhanced_label.append(pred)\n",
    "            if pred == \"AGAINST\" and ag_count < against_limit:\n",
    "        #         train_ft.append(\"{} - {} - {}\".format(tweet, pred, prob))\n",
    "                enhanced_text.append(tweet)\n",
    "                enhanced_label.append(pred)\n",
    "                ag_count += 1\n",
    "            if pred == \"FAVOR\" and fv_count < favor_limit:\n",
    "        #         train_ft.append(\"{} - {} - {}\".format(tweet, pred, prob))\n",
    "                enhanced_text.append(tweet)\n",
    "                enhanced_label.append(pred)\n",
    "                fv_count += 1\n",
    "            if pred == \"NONE\" and none_count < none_limit:\n",
    "        #         train_ft.append(\"{} - {} - {}\".format(tweet, pred, prob))\n",
    "                enhanced_text.append(tweet)\n",
    "                enhanced_label.append(pred)\n",
    "                none_count += 1\n",
    "\n",
    "        # Expand the training set\n",
    "        enhanced_text_train = enhanced_text_train + enhanced_text\n",
    "        enhanced_label_train = enhanced_label_train + enhanced_label\n",
    "        print(\"Training set new size: {}\".format(len(enhanced_text_train)))\n",
    "\n",
    "\n",
    "        vectorizer_enhanced = CountVectorizer(\n",
    "            binary=True, min_df=0.0075, max_df=0.75, ngram_range=(1, 5),\n",
    "        )\n",
    "\n",
    "        X_enhanced = vectorizer_enhanced.fit_transform([*enhanced_text_train, *textvalid, *texttest])\n",
    "        # VEC_train = vectorizer.fit_transform([*texttrain])\n",
    "        # VEC_valid = vectorizer.transform([*textvalid])\n",
    "        # VEC_test = vectorizer.transform([*texttest])\n",
    "\n",
    "        # print(X.shape)\n",
    "\n",
    "        VEC_train_enhanced = X_enhanced[:len(enhanced_text_train)]\n",
    "        VEC_valid_enhanced = X_enhanced[len(enhanced_text_train):len(enhanced_text_train) + len(textvalid)]\n",
    "        VEC_test_enhanced = X_enhanced[len(enhanced_text_train) + len(textvalid):len(enhanced_text_train) + len(textvalid) + len(texttest)]\n",
    "\n",
    "        svc_enhanced = svm.SVC(probability=True, kernel=kernel, gamma='scale')#, class_weight='balanced')\n",
    "        svc_enhanced.fit(VEC_train_enhanced, enhanced_label_train)\n",
    "        svc_enhanced.score(VEC_train_enhanced, enhanced_label_train)\n",
    "\n",
    "        svc_enhanced.score(VEC_test_enhanced, labeltest)\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        for vec_test, label in zip(VEC_test_enhanced, labeltest):\n",
    "            predictions.append(svc_enhanced.predict(vec_test)[0])\n",
    "            labels.append(label)\n",
    "\n",
    "    #     print(\"Iteration {}\".format(i))\n",
    "        f1 = f1_score(labels, predictions, average=None, labels=[\"AGAINST\", \"FAVOR\", \"NONE\"])\n",
    "        f1_av[i] += (f1[0] + f1[1]) / 2\n",
    "        f1_against[i] += f1[0]\n",
    "        f1_favor[i] += f1[1]\n",
    "        macro[i] += f1_score(labels, predictions, average=\"macro\", labels=[\"AGAINST\", \"FAVOR\", \"NONE\"])\n",
    "        micro[i] += f1_score(labels, predictions, average=\"micro\", labels=[\"AGAINST\", \"FAVOR\", \"NONE\"])\n",
    "        acc[i] += accuracy_score(labeltest, predictions)\n",
    "        prec = precision_score(labels, predictions, average=None, labels=[\"AGAINST\", \"FAVOR\", \"NONE\"])\n",
    "        prec_against[i] += prec[0]\n",
    "        prec_favor[i] += prec[1]\n",
    "        prec_av[i] += (prec[0] + prec[1]) / 2\n",
    "        prec_macro[i] += precision_score(labels, predictions, average=\"macro\", labels=[\"AGAINST\", \"FAVOR\", \"NONE\"])\n",
    "        rec = recall_score(labels, predictions, average=None, labels=[\"AGAINST\", \"FAVOR\", \"NONE\"])\n",
    "        rec_against[i] += rec[0]\n",
    "        rec_favor[i] += rec[1]\n",
    "        rec_av[i] += (rec[0] + rec[1]) / 2\n",
    "        rec_macro[i] += recall_score(labels, predictions, average=\"macro\", labels=[\"AGAINST\", \"FAVOR\", \"NONE\"])\n",
    "\n",
    "        corpus_number = (corpus_number + 1) % 20\n",
    "        f = open(\"random_partitions_of_unlabeled_corpus/abortion_unlabeled_sample_{}.csv\".format(corpus_number), 'r')\n",
    "        predictions = []\n",
    "        predictions_probs = []\n",
    "        tweets = []\n",
    "        for idx, line in enumerate(f):\n",
    "            tweet = line.replace('\\n','')\n",
    "            tweets.append(tweet)\n",
    "            Y = vectorizer_enhanced.transform([tweet])\n",
    "            predictions.append(svc_enhanced.predict(Y)[0])\n",
    "            predictions_probs.append(svc_enhanced.predict_proba(Y)[0])\n",
    "\n",
    "        tweet_prob_pred = zip(tweets, predictions_probs, predictions)\n",
    "        tweet_prob_pred = sorted(tweet_prob_pred, key=lambda x: -highest_prob(x[1]) if(count_above_threshold(x[1], 0.25) == 1) else 1)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"{},{},{},{},{},{},{},{},{},{},{},{},{},{}\".format(macro[i] / 5, f1_av[i] / 5, micro[i] / 5, f1_against[i] / 5, f1_favor[i] / 5, acc[i] / 5, prec_against[i] / 5, prec_favor[i] / 5, prec_av[i] / 5, prec_macro[i] / 5, rec_against[i] / 5, rec_favor[i] / 5, rec_av[i] / 5, rec_macro[i] / 5))\n",
    "\n",
    "#     tweet_prob_pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Get precision and recall metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
