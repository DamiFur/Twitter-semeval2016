{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from torchtext.data import TabularDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traemos los datos de mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "dbname = \"aboriton\"\n",
    "\n",
    "db = myclient[dbname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damifur/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenemos 0.61M tweets (0.46M RTs)\n"
     ]
    }
   ],
   "source": [
    "base_query = {}\n",
    "rt_query = {**base_query, **{\"tweet.retweeted_status\": {\"$exists\": True}}}\n",
    "\n",
    "\n",
    "tweets = db.abortion.find(base_query)\n",
    "rts = db.abortion.find(rt_query)\n",
    "\n",
    "\n",
    "print(\"Tenemos {:.2f}M tweets ({:.2f}M RTs)\".format(tweets.count() / 1e6, db.abortion.count_documents(rt_query) / 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de retwitteros : 219183\n"
     ]
    }
   ],
   "source": [
    "cantidad_de_usuarios = len(db.abortion.find(rt_query).distinct('user'))\n",
    "\n",
    "print(\"Cantidad de retwitteros : {}\".format(cantidad_de_usuarios))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grafo de Retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "full_g = nx.Graph()\n",
    "vertex_ids = set()\n",
    "\n",
    "\n",
    "for retweet in rts:\n",
    "    original_tweet = retweet[\"tweet\"][\"retweeted_status\"]\n",
    "    full_g.add_edge(retweet[\"user\"], original_tweet[\"user\"][\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodos: 0.23M\n",
      "Ejes: 0.40M\n"
     ]
    }
   ],
   "source": [
    "n = len(full_g.nodes)\n",
    "m = len(full_g.edges)\n",
    "complete = n * (n-1) / 2\n",
    "\n",
    "print(\"Nodos: {:.2f}M\".format((n / 1e6)))\n",
    "print(\"Ejes: {:.2f}M\".format((m / 1e6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "degrees = np.array([full_g.degree(node) for node in full_g.nodes()])\n",
    "\n",
    "plt.hist(degrees, range=(0, 200), bins=100);\n",
    "plt.semilogy();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nos quedan 0.02M nodos (antes eran 0.23M)\n",
      "Componentes conexas = 875\n"
     ]
    }
   ],
   "source": [
    "def subgraph_strong_nodes(g, n):\n",
    "    \"\"\"\n",
    "    Devuelve subgrafo con nodos de grado mayor a n\n",
    "    \"\"\"\n",
    "    good_nodes = [node for node in g.nodes() if g.degree(node) >= n]\n",
    "    \n",
    "    return g.subgraph(good_nodes)\n",
    "\n",
    "g = subgraph_strong_nodes(full_g, 5)\n",
    "\n",
    "print(\"Nos quedan {:.2f}M nodos (antes eran {:.2f}M)\".format(\n",
    "    len(g.nodes) / 1e6,\n",
    "    len(full_g.nodes) / 1e6\n",
    "))\n",
    "print(\"Componentes conexas = {}\".format(len(list(nx.connected_components(g)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Componentes conexas = 875\n",
      "Componente 0  --> 19000  usuarios\n",
      "Restantes     --> 923    usuarios\n"
     ]
    }
   ],
   "source": [
    "components = list(nx.connected_components(g))\n",
    "\n",
    "print(\"Componentes conexas = {}\".format(len(components)))\n",
    "\n",
    "print(\"Componente 0  --> {:<6} usuarios\".format(len(components[0])))\n",
    "\n",
    "resto = [len(components[i]) for i in range(1, len(components))]\n",
    "print(\"Restantes     --> {:<6} usuarios\".format(sum(resto)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the biggest component. Usually it's the first but not necessarily\n",
    "g = g.subgraph(components[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corremos Louvain para encontrar las comunidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resoluci√≥n 1.00---> 28 particiones (10 con m√°s de 100 usuarios)\n",
      "resoluci√≥n 1.50---> 19 particiones (8 con m√°s de 100 usuarios)\n",
      "resoluci√≥n 2.00---> 11 particiones (7 con m√°s de 100 usuarios)\n",
      "resoluci√≥n 3.00---> 10 particiones (5 con m√°s de 100 usuarios)\n",
      "resoluci√≥n 5.00---> 6 particiones (4 con m√°s de 100 usuarios)\n",
      "resoluci√≥n 6.00---> 6 particiones (4 con m√°s de 100 usuarios)\n",
      "resoluci√≥n 7.00---> 6 particiones (4 con m√°s de 100 usuarios)\n",
      "resoluci√≥n 10.00---> 5 particiones (3 con m√°s de 100 usuarios)\n",
      "resoluci√≥n 15.00---> 4 particiones (3 con m√°s de 100 usuarios)\n"
     ]
    }
   ],
   "source": [
    "import cylouvain\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "user_threshold = 100\n",
    "\n",
    "for resolution in  [1, 1.5, 2, 3, 5, 6, 7, 10, 15]:\n",
    "    begin = time.time()\n",
    "    print(\"resoluci√≥n {:.2f}\".format(resolution), end=\"\")\n",
    "    \n",
    "    partition = cylouvain.best_partition(g, resolution=resolution)\n",
    "    end = time.time()\n",
    "    \n",
    "    num_partitions = len(set(partition.values()))\n",
    "    counter = Counter(partition.values())\n",
    "    \n",
    "    \n",
    "    important_partitions = len([_ for x in counter.items() if x[1] > user_threshold])\n",
    "    \n",
    "    print(\"---> {} particiones ({} con m√°s de {} usuarios)\".format(\n",
    "        num_partitions,\n",
    "        important_partitions,\n",
    "        user_threshold,\n",
    "    ))\n",
    "    #print(\"Tiempo: {:.2f} minutos\".format((end - begin) / 60))\n",
    "    partitions[resolution] = partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modularity = 10.0\n",
    "partition = partitions[modularity]\n",
    "partition_to_ids = {partition_number:[] for partition_number in range(len(set(partition.values())))}\n",
    "\n",
    "num_partitions = len(set(partition.values()))\n",
    "for user_id, user_partition in partition.items():\n",
    "    partition_to_ids[user_partition].append(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_users = {}\n",
    "def get_user(user_id):\n",
    "    if user_id not in cache_users:\n",
    "        user = db.users.find_one({\"id\": user_id})\n",
    "        if user:\n",
    "            cache_users[user_id] = user\n",
    "        else:\n",
    "            cache_users[user_id] = None\n",
    "    return cache_users[user_id] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from tweepy import TweepError\n",
    "from collections import Counter\n",
    "\n",
    "users_with_tweets = db.abortion.find({\n",
    "    **base_query\n",
    "}).distinct('user')\n",
    "    \n",
    "\n",
    "    \n",
    "def most_followed_by(followers, N=5):\n",
    "    \"\"\"\n",
    "    Returns the ids of the users most retweeted by followers\n",
    "    \n",
    "    Arguments:\n",
    "    ---------\n",
    "    \n",
    "    followers: iterable of dicts/user documents\n",
    "        The dicts should have the key 'friends_ids' \n",
    "        \n",
    "    N: int (default=5)\n",
    "        Number of top-followed users to return\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    fd = nltk.FreqDist()\n",
    "    \n",
    "    for follower in followers:\n",
    "        try:\n",
    "            for fid in follower[\"friends_ids\"]:\n",
    "                fd[fid] += 1\n",
    "        except KeyError as e:\n",
    "            \"\"\" No tiene friends_ids \"\"\"\n",
    "            continue\n",
    "        \n",
    "    return fd.most_common(N)\n",
    "\n",
    "def most_retweeted(users, N=5):\n",
    "    user_ids = [u[\"id\"] for u in users]\n",
    "    \n",
    "    retweets = db.abortion.find({\n",
    "        **rt_query,\n",
    "        **{\n",
    "            \"user\": {\"$in\": user_ids},\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    counter = Counter(tweet[\"tweet\"][\"retweeted_status\"][\"user\"][\"id\"] for tweet in retweets)\n",
    "    \n",
    "    return counter.most_common(N)\n",
    "    \n",
    "    \n",
    "\n",
    "def users_of_community(db, partition_number, has_tweets=True):    \n",
    "    user_ids = partition_to_ids[partition_number]\n",
    "    \n",
    "    if has_tweets:\n",
    "        user_ids = set(user_ids).intersection(users_with_tweets)\n",
    "    return db.users.find({\n",
    "        \"id\": {\"$in\": list(user_ids)}\n",
    "    })\n",
    "\n",
    "\n",
    "def community_info(partition_number, \n",
    "                   num_users=5, num_tweets=5, followers=10,\n",
    "                   user_threshold=10):\n",
    "    users = list(users_of_community(db, partition_number))\n",
    "    \n",
    "    if len(users) <= user_threshold:\n",
    "        return\n",
    "    \n",
    "    print((\"*\" * 40 + '\\n') * 4)\n",
    "    print(\"Partici√≥n {}\".format(partition_number))\n",
    "    print(\"Cantidad de usuarios: {}\".format(len(users)))\n",
    "\n",
    "    \n",
    "    user_ids = [user[\"id\"] for user in users]\n",
    "    num_tweets = db.abortion.count_documents({\n",
    "        **base_query,\n",
    "        **{\"user\": {\"$in\": user_ids}}\n",
    "    })\n",
    "    print(\"Cantidad de tweets: {}\".format(num_tweets))\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"\\n\\nUsuarios m√°s seguidos\\n\\n\")\n",
    "    for i, (user_id, count) in enumerate(most_followed_by(users, followers)):\n",
    "        try:\n",
    "            user = get_user(user_id)\n",
    "            print(\"{0:<2}. {2:<35} (@{1:<20}) -- {3:<6} seguidores\".format(i+1, user[\"screen_name\"], user[\"name\"], count))\n",
    "        except TweepError as e:\n",
    "            print(\"{} - {}\".format(user_id, e))\n",
    "    \"\"\"\n",
    "    print(\"\\n\\nUsuarios m√°s RT \\n\\n\")\n",
    "    for i, (user_id, count) in enumerate(most_retweeted(users, followers)):\n",
    "        try:\n",
    "            user = get_user(user_id)\n",
    "            if user:\n",
    "                print(\"{0:<2}. {2:<22} (@{1:<15}) -- {3:<6} RT\".format(\n",
    "                    i+1, \n",
    "                    user[\"screen_name\"][:20], \n",
    "                    user[\"name\"][:15], \n",
    "                    count))\n",
    "        except TweepError as e:\n",
    "            print(\"{} - {}\".format(user_id, e))\n",
    "    \n",
    "    \n",
    "    for user in users[:num_users]:\n",
    "        print((('=')*50 + '\\n')*3)\n",
    "\n",
    "        print(\"{1} (@{0})\".format(user['screen_name'], user['name']))\n",
    "        print(\"Ubicaci√≥n: \", user['location'])\n",
    "        print(\"Descripci√≥n: \", user['description'])\n",
    "        print(user['id'])\n",
    "        \n",
    "        query = {\n",
    "            **base_query,\n",
    "            **{\n",
    "                \"tweet.user.id\": user[\"id\"], \n",
    "            }\n",
    "        }\n",
    "        \n",
    "        tweets = db.abortion.find(query).limit(num_tweets)\n",
    "        \n",
    "        print(\"\\nTweets:\")\n",
    "        \n",
    "        for tweet in tweets:\n",
    "            if 'tweet' in tweet and 'full_text' in tweet['tweet']:\n",
    "                print(tweet['tweet']['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "****************************************\n",
      "****************************************\n",
      "****************************************\n",
      "\n",
      "Partici√≥n 1\n",
      "Cantidad de usuarios: 13044\n",
      "Cantidad de tweets: 165422\n",
      "\n",
      "\n",
      "Usuarios m√°s RT \n",
      "\n",
      "\n",
      "1 . LifeNews.com           (@LifeNewsHQ     ) -- 16326  RT\n",
      "2 . Live Action            (@LiveAction     ) -- 4603   RT\n",
      "3 . Charlie Kirk           (@charliekirk11  ) -- 3894   RT\n",
      "4 . üá∫üá∏üë∂‚úùÔ∏èI am ProLi        (@Prolife_IAM    ) -- 3577   RT\n",
      "5 . Abby Johnson           (@AbbyJohnson    ) -- 2144   RT\n",
      "6 . Judicial Watch         (@JudicialWatch  ) -- 2039   RT\n",
      "7 . March for Life         (@March_for_Life ) -- 2022   RT\n",
      "8 . LAYLA {‚≠ê}              (@LaylaAlisha11  ) -- 1991   RT\n",
      "9 . Lila Rose              (@LilaGraceRose  ) -- 1990   RT\n",
      "10. Fr. Frank Pavon        (@frfrankpavone  ) -- 1986   RT\n",
      "11. Educating Liber        (@Education4Libs ) -- 1612   RT\n",
      "12. Cori #Trump2020        (@Cocofritz      ) -- 1530   RT\n",
      "13. Rod The Nationa        (@RodHillis1     ) -- 1489   RT\n",
      "14. Clint Eastwoodüá∫        (@ClintEastwoodLA) -- 1479   RT\n",
      "15. Trump War Room         (@TrumpWarRoom   ) -- 1161   RT\n",
      "16. Evan Kilgore           (@EvanAKilgore   ) -- 1141   RT\n",
      "17. Obianuju Ekeoch        (@obianuju       ) -- 1037   RT\n",
      "18. Senator Ted Cru        (@SenTedCruz     ) -- 944    RT\n",
      "19. Students for Li        (@StudentsforLife) -- 911    RT\n",
      "20. Jay Sekulow            (@JaySekulow     ) -- 853    RT\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Erick Erickson (@EWErickson)\n",
      "Ubicaci√≥n:  Atlanta, GA\n",
      "Descripci√≥n:  Romans 1:16. Writer @Resurgent. Talker @WSBRadio. Speaker (http://premierespeakers.com/contact). Working on my PhD in Theology.\n",
      "640893\n",
      "\n",
      "Tweets:\n",
      "As Kentuckians go to the polls today , nothing less than human dignity is at the ballot box . ‚Å¶ The contrast between candidates is clear : One affirms a culture of life , the other would work against it . https://t.co/uawdKWrWWD\n",
      "This ruling discards even the most basic protections of religious liberty in favor of extremist abortionist demands . Health care professionals who oppose abortion on moral or religious grounds should NEVER be forced to participate in infanticide . https://t.co/56YsHsrOy3\n",
      "If a pro-life group runs an ad saying \" choose life \" that's obviously not a political ad . But the odds that Twitter would axe it seem at least 50 % .\n",
      "\" So-called conscience ... \" The major media simply hate religious conservatives , and they can't even get through a headline without letting it show . https://t.co/ebNpNz09CU\n",
      "Even though this is not a good final state situation , if Dems said ‚Äú abortion should be decided at the state level and compelled speech on sexuality is bad and Christian schools are fine ‚Äù we ‚Äô d be living in a different political universe . https://t.co/iijinzmPdW\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "The New York Times (@nytimes)\n",
      "Ubicaci√≥n:  New York City\n",
      "Descripci√≥n:  News tips? Share them here: https://t.co/ghL9OoYKMM\n",
      "\n",
      "\"The Weekly\" is our new TV series. Episodes air Sundays at 10 p.m. on FX and on Hulu the next day.\n",
      "807095\n",
      "\n",
      "Tweets:\n",
      "Missouri investigators compiled a spreadsheet of patient data , including menstrual period dates , seeking evidence of failed abortions at a St . Louis Planned Parenthood during months of legal wrangling that could close the clinic , the last one in the state https://t.co/GgezBX5QBE\n",
      "A furor over patient privacy shadowed a 4 - day hearing this week that will help determine the fate of the last remaining abortion clinic in Missouri https://t.co/lZCiGjwpSB\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Kevin Maher (@kevinm13)\n",
      "Ubicaci√≥n:  New Jersey, USA\n",
      "Descripci√≥n:  Conservative, MAGA, Dad, Catholic, Pro-life, opinionated old fart.    At least 7th generation American (since 1720). Have a wonderful big family.  RT often.\n",
      "823420\n",
      "\n",
      "Tweets:\n",
      "I do not for one minute believe the majority of voters in Virginia voted for the party of blackface & extreme abortion up to birth.Not for one minute . I smell #VoterFraud . And This is the ONLY way Trump could possibly lose in 2020 . #VoterIDNow #VAelections #StopVoterFraud https://t.co/VTHUo3o0TH\n",
      "This is your daily reminder that Democrats still support a Governor who : Admitted to wearing either blackface or a KKK hood ... Then lied about itWas nicknamed \" Coonman \" in collegeSupports brutal , post-birth abortionRalph Northam is a disgrace ‚Äî even by the left's standards\n",
      "1 . Hippocratic Oath = doctors promise to PRESERVE life ! üë∂ üèª 2 . Since when can you force a doctor to do a procedure they don't want to do ? üò° It's time to tell the left we don't want their satanistic policies anymore üòà Turn to God , Patriots , & pray to Him to heal our nation . üá∫ üá∏ https://t.co/A0S3AIx5S1\n",
      "Total donations to lawmakers 2012-2016 Planned Parenthood : $ 7,858 , 142NRA : $ 3,254 , 694Number of kids killed by org members last 5 yrs :P lanned Parenthood : 1,962 , 514NRA : 0Percent of annual budget funded by taxpayersPlanned Parenthood : 40 % NRA : 0 % Why aren ‚Äô t they defunded ?\n",
      "Raise your hand if you think Planned Parenthood needs to be SHUT DOWN now ! üôã ‚Äç ‚ôÄ Ô∏è üôã ‚Äç ‚ôÇ Ô∏è Planned Parenthood Partner Admits Selling Intact Livers , Lungs and Brains From Aborted Babieshttps :/ / t.co/BrciZYb3ip\n",
      "Former Abortion Clinic Owner : We Pushed Sex Ed on Kids to Create a Market for Abortion https://t.co/VX69dLtfH5\n",
      "President Trump has a been a pro-life breath of fresh air after 8 years of Obama stacking courts.We need 4 more years to get more activist judges off the bench.Who agrees ! üôã ‚Äç ‚ôÇ Ô∏è üôã ‚Äç ‚ôÄ Ô∏è President Trump Has a Wonderful Pro-Life Record on Appointing Judges https://t.co/a5JD6t46pO\n",
      "President Trump has a been a pro-life breath of fresh air after 8 years of Obama stacking courts.We need 4 more years to get more activist judges off the bench.Who agrees ! üôã ‚Äç ‚ôÇ Ô∏è üôã ‚Äç ‚ôÄ Ô∏è President Trump Has a Wonderful Pro-Life Record on Appointing Judges https://t.co/btYtDMvl7d\n",
      "This is truly bizarre and disgusting ... Drag Queens Sell Pro-Life Christians Like Slaves to Raise Funds for Planned Parenthood Abortion Biz https://t.co/2s5y3EwZK7\n",
      "President Trump is going ‚Äò gangbusters ‚Äô on pro-life : Former nurse Jill Stanek . Jill said everything we ‚Äô ve asked him to do @POTUS has done ! Trump is called the Most Pro-Life President in history ! He is focusing much of his 2020 campaign on ProLife issues ! https://t.co/aOErAjMiay\n",
      "BREAKING NEWS : Instead of holding Planned Parenthood accountable for selling baby parts , they want to convict the pro-lifers who exposed it ! Just HORRIBLE ! ! ! Judge With Planned Parenthood Ties Orders Jury to Find David Daleiden Guilty of Racketeering https://t.co/RKCYkj1a7Q\n",
      "Safe , legal & rare ? ? Now : ‚Äú Abortion providers are some of my personal heroes & modern day saints ‚Äù by the head of NAF , which accts for 1/2 the abortions in US . She also has a rite of blessing for parents about kill a baby ! #ARIELSARMY üåø #AbortionIsMurder https://t.co/CxH2aDR9AW\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Mashable (@mashable)\n",
      "Ubicaci√≥n:  None\n",
      "Descripci√≥n:  Mashable is for superfans. We're not for the casually curious. Obsess with us.\n",
      "972651\n",
      "\n",
      "Tweets:\n",
      "State abortion laws are so confusing . Planned Parenthood hopes a new digital tool can help . https://t.co/n53CWevWGw\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Patrick Ruffini (@PatrickRuffini)\n",
      "Ubicaci√≥n:  Washington, DC\n",
      "Descripci√≥n:  Pointing out the disconnect between urban professionals and the median voter is my jam.\n",
      "1050111\n",
      "\n",
      "Tweets:\n",
      "Watch everyone in DC and NYC go insane when we eventually elect a pro-life , nationalize-the-banks President .\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Cypher (@hiscity)\n",
      "Ubicaci√≥n:  Revelation 1:9\n",
      "Descripci√≥n:  radial & concentric impact pattern shows  tectonic plates are static\n",
      "1452241\n",
      "\n",
      "Tweets:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fighting an intense legal battle to keep out evidence , #PlannedParenthood's own attorney just opened the door to disturbing testimony in our trial . This is a big deal . We can defeat Planned Parenthood but we need you . Sign our petition today . https://t.co/XEzTRTb7UU\n",
      "Join me right now in Mexico where I'm on stage to debate protecting innocent human life in the womb : https://t.co/b8cgMhfJHG #prolife\n",
      "@The_War_Economy Here's his district . He's soft on border issues & abortion . There's no mystery he's IC . https://t.co/jgQAdMxPxY\n",
      "@The_War_Economy Here's his district . He's soft on border issues & abortion . There's no mystery he's IC . https://t.co/jgQAdMxPxY\n",
      ". @giannajessen to Congress : ‚Äú If abortion is merely about women ‚Äô s rights , then where were mine ? ‚Äù Human rights begin in the womb . We must protect the rights of ALL women ‚Äî born and unborn . #ProLife #ChooseLife üíì üë£ https://t.co/nyxmls9ACJ\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Rhianna Rita Starr (@rhianna)\n",
      "Ubicaci√≥n:  New York, NY\n",
      "Descripci√≥n:  Tarot card reader with interest in Future Options; Flamenco enthusiast; History & Vampire buff; Loves art history & museums; Current Events Junkie\n",
      "2570001\n",
      "\n",
      "Tweets:\n",
      "Violence against peaceful pro-life protests are on the rise . #Prolife #FreedomofExpression #FreeSpeech #Violence #Prochoice #Abortion https://t.co/r6BqGivJyx\n",
      "‚Äú oh baby , I love you , but you know we're both not ready for a baby ‚Äù These are very common words spoken to ü§∞ üèΩ women by that boyfriend who just wanted to ‚Äú have some fun ‚Äù , the words that lead many to the dark doorsteps of the abortionist.Abortion kills babies & hurts women . https://t.co/IydaVSnk3G\n",
      "The hounding of pro-life protesters shatters the myth of ' tolerant ' Britain https://t.co/0IJnpBq2Hh via @telegraphnews [ You may need to sign in to read this , but you can do so for free . Please Retweet ]\n",
      "So honoured to have been a part of this awesome video . #HumanRights #Prolife #Abortion https://t.co/8KOAQvPAuA\n",
      "Excellent article üëè We are supposed to be a society that protects freedom of religion , rather than crushing it beneath an insidious , faux-liberal agenda . But try telling that to Christian Harkinghttps :/ / t.co/CfgetcSiBu#Prayer #Prolife #ReligiousFreedom\n",
      "A 13 year old girl found this poster in her high school gym locker room . The only answer Planned Parenthood will give is how much the abortion is and when to show up to kill your child without telling your parents.EVIL ! ! ! @prolife #defundpp https://t.co/xc6kpMYBBw\n",
      "I totally Agree @dlongenecker1 ! It's time to change the fear of preaching about abortion into a fear of what will happen if we don't ! #prolife https://t.co/AUN7YIbO5Q\n",
      "Why Joe Biden being denied communion is a big deal for people of faith . Here is my debate with Christopher Hale that aired today on Foxnews . #prolife @JoeBiden https://t.co/KcKoTDqjd5\n",
      "New head of National #Abortion Federation is an Episcopal priest , Rev . Katherine Ragsdale ! ‚Äú Throughout my career , I have preached that abortion is a blessing & that providers are modern-day saints and heroes , \" she says.Can anyone say \" false prophet ? \" #Christian #prolife\n",
      "No @MSNBC We oppose Kamala Harris because she is a pro-abortion radical who supports killing babies , targets pro-life people and is unfit to be president ... MSNBC Host Says Kamala Harris ‚Äô Campaign is Flopping Because Voters are Too Racist https://t.co/oG9CVibqYT\n",
      "Sorry @PinkBut you can't claim to \" care bout kids \" if you support killing them in abortions ! Singer Pink Wins Award for Supporting Killing Babies Abortion , But Claims ‚Äú I Care About Kids ‚Äù https://t.co/6wneuZHr6C\n",
      "Let me just put this thought out there ... no child ever dreams of being an abortion worker when they grow up . https://t.co/sAs5QZD1t0\n",
      "Sorry @JoeBiden We won't support you or any pro abortion candidate.Raise your hand if you agree ! Joe Biden Tries to Win Over Catholic Voters , But He Supports Abortions Up to Birth https://t.co/n59K1JJBWx\n",
      "Today is Fr . Frank's 31st Anniversary of Priestly Ordination ! We at the Priests for Life Team , We wish you another 31 years of service to Christ ! Thank you for your dedication fighting for the unborn babies that have no voice ! Let Father know you are thinking of him ! #prolife https://t.co/pGTzUIyh4o\n",
      "No @MSNBC We oppose Kamala Harris because she is a pro-abortion radical who supports killing babies , targets pro-life people and is unfit to be president ... MSNBC Host Says Kamala Harris ‚Äô Campaign is Flopping Because Voters are Too Racist https://t.co/JFHetqfyjf\n",
      "Absolutely right . And it won't just be for the extremely sick . Check the parallels with abortion . Poverty plays a huge role . I'd be interested to know the reasons behind if any #prochoice advocates were against this , though . https://t.co/ojompF7QfS\n",
      "Worth repeating , if you believe #abortionishealthcare then data should be collected from all states with the same precision that any other healthcare is . This is one of the points where #abortionishealthcare and #abortionisnothealthcare should agree . Concrete stats are important .\n",
      "A must-read article from @LilaGraceRose and Dr . Donna Harrison : \" Killing the baby offers no medical advantage to the mother . \" #AbortionIsNotHealthcarehttps :/ / t.co/reB4sjxGO5\n",
      "The common refrain is that #abortion is between a woman and her doctor . But most women don't go to their own doctors , they go to free-standing abortion businesses run by the lowest rung of the medical profession . #TellTheTruth #Abortionisnothealthcare\n",
      "After #abortion you will never see the abortionist again ( unless you go back for another abortion ) . No one will meet with a family member to explain your recovery . You will be told to expect bleeding and cramps . No painkillers . No antibiotics . No baby . #Abortionisnothealthcare\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "∆¨–Ñ∆õ∆ì∆õ∆ù ∆¶–Ñ∆ñ‘º‘º∆≥ üç∏ (@velvethammer)\n",
      "Ubicaci√≥n:  üá∫üá∏'Merica bitches!\n",
      "Descripci√≥n:  12th gen American whose ancestors fought in Revolutionary War\n",
      "‚úùÔ∏è‚ôÄÔ∏èMilitary Brat Civilizationist Freeform Citizen Journo\n",
      "#1a #2a Pronouns: feminine normal\n",
      "5145071\n",
      "\n",
      "Tweets:\n",
      "@babcox_joey @velvethammer @ewarren Guess what @ewarren if they are pregnant and need an abortion their reproductive system is working . It ‚Äô s bad choices that lead to abortion . So get that straight\n",
      "If you are outraged about the attack on the #BabyTrumpBalloon , but aren't outraged about children in garbage cans at Planned Parenthood , you are a rank infanticidal hypocrite with misplaced priorities .\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Emily Zanotti‚Äôs Great & Unmatched Wisdom (@emzanotti)\n",
      "Ubicaci√≥n:  Chicago, IL\n",
      "Descripci√≥n:  Writer, crafter, politico, nerd. Sr. ed. @realDailyWire. WWII historian. Catholic. Twin mum. Cat lady. @Ladybrainscast-er. Opinions my own but should be yours.\n",
      "5447242\n",
      "\n",
      "Tweets:\n",
      "I still see those plastic-wrapped bodies in my nightmares . They turned me from casually pro-life to a hardcore crusader , as did the way I felt when I was just newly pregnant and then I suddenly wasn ‚Äô t anymore ...\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Keith Mills (@KeithMillsD7)\n",
      "Ubicaci√≥n:  Limerick & Dublin, Ireland\n",
      "Descripci√≥n:  Dublin based proud Limerick man.  Blocked by Gemma O'Doherty,  Dustin and other assorted Turkeys, too hot for the @ireland account!\n",
      "#andapedestrian\n",
      "6268872\n",
      "\n",
      "Tweets:\n",
      "@FaithinDepth @bazyjonesy @MaryKenny4 The pro-life side didn't really have a chance to win but it had little to do with the media . It was due to bad decisions made over the previous 25 years . The result could have been a lot closer with different tactics .\n",
      "****************************************\n",
      "****************************************\n",
      "****************************************\n",
      "****************************************\n",
      "\n",
      "Partici√≥n 2\n",
      "Cantidad de usuarios: 129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tweets: 1061\n",
      "\n",
      "\n",
      "Usuarios m√°s RT \n",
      "\n",
      "\n",
      "1 . charity Mukuba         (@mukuba_charity ) -- 85     RT\n",
      "2 . Wuod Japuonj           (@OsumbaJM       ) -- 57     RT\n",
      "3 . Amos Kilanya           (@AmosKilanya    ) -- 42     RT\n",
      "4 . Digital Nativeüá∞        (@EmodiaBenjamin ) -- 41     RT\n",
      "5 . Monica Nyaga.          (@nyagamoniq     ) -- 41     RT\n",
      "6 . Lilianüá∞üá™               (@PstLily        ) -- 40     RT\n",
      "7 . Nelson Amenya          (@amenya_nelson  ) -- 37     RT\n",
      "8 . Julius üá∞üá™              (@J_Kikwai       ) -- 28     RT\n",
      "9 . Pizzahmwitu            (@pizzahmwitu    ) -- 19     RT\n",
      "10. Kibet Vincent R        (@KibetVincentRo1) -- 18     RT\n",
      "11. Susan Wanjiru          (@swanjirul      ) -- 15     RT\n",
      "12. Catholic MPs Ke        (@CatholicmpsKE  ) -- 11     RT\n",
      "13. Bshp Albert            (@BshpAlbert     ) -- 11     RT\n",
      "14. Charity                (@charity_sang   ) -- 11     RT\n",
      "15. Sir Ian                (@ianowilid      ) -- 10     RT\n",
      "16. willie ngugi           (@williengugi3   ) -- 10     RT\n",
      "17. Arthur Park            (@ArthurPark60   ) -- 9      RT\n",
      "18. Merylyne Koech         (@MerylyneKoech  ) -- 9      RT\n",
      "19. Kipchumba Joel         (@KipchumbaJoel2 ) -- 9      RT\n",
      "20. Arthur                 (@ArthurPstr     ) -- 9      RT\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Nahashon Kimemia (@nahashon87)\n",
      "Ubicaci√≥n:  Nairobi, Kenya\n",
      "Descripci√≥n:  Fight to the death for truth & the Lord God will war on your side. -Sirach 4:28.\n",
      "IG: nahashonkimemia\n",
      "YouTube: Nahashon Kimemia\n",
      "FB: The Truth by Nahashon Kimemia\n",
      "39292745\n",
      "\n",
      "Tweets:\n",
      "Nasikia Kenya will be hosting a pro-abortion conference soon . They'll not support it directly , but chini ya maji . Ati China will be giving tips on population control , lol . Feminists wamekuwa wakilala as Kenyan women struggle wacha sasa uone wakiamka during hiyo conference .\n",
      "@nahashon87 sometimes they hide it as PRO-CHOICE or REPRODUCTION RIGHTS which is mainly HEAVILY funded by USA democrats . MARIE STOPES is a case in point and PLANNED PARENTHOOD .\n",
      "@nahashon87 Watu mko story za birth control na abortion yet most of the 90 ' s babies wako barren ... Naskia vaccines zimeaffect watu wengi sana ... Hawajui tu ju wanangoja wafike 30 ndio wazae\n",
      "For those who don't know , reproductive rights is the politically correct term for abortion . The UN has tried to control the population of African nations for the longest time . Thus far , it has failed . It'll keep failing , but that doesn't mean it'll stop trying . #NairobiSummit\n",
      "Shida zetu ziko hapa si hii myth ya overpopulation . \" National wealth also reflects productivity , which depends on technology , education , health , business regulatory climate , & economic policies . \" There's no economic or moral justification for abortion . #NairobiSummit\n",
      "History will not be kind to Uhuru1.He failed to unite us2.He wrecked an economy3.He sold us to China through debt4.He sought to keep power through BBI5.He is opening Kenya to abortion on demand6.He used DCI , DPP & KRA to target his opponents #LetterToTheState #NairobiSummit\n",
      "\" We don ‚Äô t want the word to go out that we want to exterminate the Negro population . \" - Margaret Sanger , founder of Planned Parenthood , in her letter to Dr . Clarence J . Gamble , December 10 , 1939 . #LetterToTheState #NairobiSummit\n",
      "‚Äú I accepted an invitation to talk to the women's branch of the Ku Klux Klan ... I believed I had accomplished my purpose . A dozen invitations to speak to similar groups were proffered . ‚Äù - Margaret Sanger , founder of Planned Parenthood . #LetterToTheState #NairobiSummit\n",
      "While black women , the most fertile women in the US , have accepted this culture of abortion , white women have not . - White women , the least fertile , had the highest number of live births in NY , 39,112 & 2nd lowest no of abortions recorded , 9,704 . #LetterToTheState #NairobiSummit\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Lilianüá∞üá™ (@PstLily)\n",
      "Ubicaci√≥n:  Nairobi, Kenya\n",
      "Descripci√≥n:  Who ARE these heavenly hosts walking the earth doing signs and wonders? Prepare;Christ #JESUS,The MESSIAH is Coming http://Jesusislordradio.info #ProphetDrOwuor\n",
      "85238236\n",
      "\n",
      "Tweets:\n",
      "Sin Is Sin and it separates a people and a nation form God ! Never can a state sit and Discuss an Agenda promoting Homosexuality and Abortion ! No ! These we reject ! #LetterToTheState https://t.co/zgion2Bz59\n",
      "#LetterToTheStateLet me ask you If abortion was legal at a time when our mothers gave birth to us , would we be born today ? Who would be the president of the country , the doctor or the teacher ? I say no to abortions and lesbianism\n",
      "The Bible is Clear , Murder is sin and those who practise it they have their place in the eternal Hell fire ! I am asking today , why then legalize abortion ? Aren't you aware the Bible warns against killing ? #LetterToTheState\n",
      "We as Kenyans are totally outraged at the glorification of homosexuality and abortion by the @unon and western powers ! we refuse and reject this campaign because our moral standards dont allow this barbaric acts that are against our beliefs and culture . #LetterToTheState\n",
      "The LORD , through the teachings and instructions dispensed by His most trusted Messengers on earth has clearly shown this generation the way to His glorious Kingdom - Righteousness & Holiness . If we glorify homosexuality and abortion , that is lunacy of the heart ! #LetterToTheState\n",
      "A nation that is founded on GOD should not even have this as a discussion.The national anthem states it well , Oh LORD of all creation ! HE IS LORD and HE IS HOLY . And as a citizen of this blessed nation I also say no.Abortion is Sin , Homosexuality is Sin ! #LetterToTheState https://t.co/wRVoGVC7c2\n",
      "Well said , may the church of Christ and all who fear God hear this and say no to abortion and homosexuality and every form of rebellion against God's law #LetterToTheState https://t.co/8xGqj4Bk4C\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Patrick Kiprono (@patrkip)\n",
      "Ubicaci√≥n:  Kenya\n",
      "Descripci√≥n:  Husband|Father|Teacher|I love JESUS.üá∞üá™\n",
      "256474168\n",
      "\n",
      "Tweets:\n",
      "https://t.co/fI3o7oirLO\n",
      "Hi , I signed a petition to Her Excellency Monica Juma which says : \" Reject Pro-Abortion and Sexualization Agenda at ICPD + 25 Nairobi Summit \" Will you sign this petition ? Click here : https://t.co/Dreq84Z7LT Thanks ! ‚Äã #StopPoliceBrutality #jkuatlivesmatter #MainaAndKingangi\n",
      "#LetterToTheState The GOD of Israel who created Kenya , can't accept laws that contradict His word . Let us now bow down to Him alone by standing firm and obey His word . Legalising abortion is sin !\n",
      "No RATIONAL being can PUBLICLY advocate for HOMOSEXUALITY and / or ABORTION . If they do , then it only points to how degenerated such people areThe acts are UGLY , CRUEL & excessively SATANICAs for Human Rights , we refuse those that tell us to be GAYS #LetterToTheState\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Jennifer  Shamalla (@JeniferShamalla)\n",
      "Ubicaci√≥n:  Africa\n",
      "Descripci√≥n:  Member of Parliament. Lawyer who believes that all that is necessary for evil to triumph is for good women to do nothing\n",
      "268934302\n",
      "\n",
      "Tweets:\n",
      "The Prolife and Family Friendly conference will take place on Mon 11th ‚Äì Thur 14th Nov 2019 at the Cardinal Otunga Plaza Nairobi . #ICPD25 . #KenyaForFamily . @HonChrisWamalwa @CUEA_OFFICIAL @annmtave @CitizenGO @Radio_Waumini https://t.co/RHcghWtZsd\n",
      "Religious leaders who gathered for a pre - summit on the conference say deliberations on universal access to Sexual and Reproductive Health and Rights may provide a platform for the discussion of what they term as alien practices such as homosexuality and abortion #KTNPrime https://t.co/lIN0RxBZ75\n",
      "Loving my shirt . #ICPD25DOESNOTREPRESENTME as we show the unplanned Movie today marking the beginning of pro-life side events . #IMarchForLife @NairobiSummit @UNFPA https://t.co/2OJ3pYEiNV\n",
      "ICPD 25 - THE HOLY SEE SPEAKSThe ICPD and its encompassing Programme of Action within the international community ‚Äô s broad development agenda should not be reduced to so-called ‚Äú sexual and reproductive health and rights ‚Äù and ‚Äú comprehensive sexuality education . ‚Äù #KenyaForFamily\n",
      "ICPD 25 Our Constitution doesn't allow abortion and those other evil issues being brought to us by ICPD 25 . From the statement of our president ICPD is dead on arrival ! Dr . Chris Wamalwa , CAMPSSI Chairman & Deputy whipKenya National Assembly @HonChrisWamalwa @Jude_Njomo https://t.co/h49OmQ8C3r\n",
      "This youth declaration calls for the legalization of abortion , prostitution ( sex work ) , same-sex marriage , CSE and SRH services foryouth without parental consent . UNFPA and IPPF plan to advance this controversial SRHagenda even further through youth at the #ICPDNairobiSummit\n",
      "This youth declaration calls for the legalization of abortion , prostitution ( sex work ) , same-sex marriage , CSE and SRH services for youth without parental consent . UNFPA and IPPF plan to advance this controversial SRH agenda even further through youth at the #ICPDNairobiSummit\n",
      "ICPD 25 Powerful statement from Prolife and Pro-Family envoys and governments representives.They all agreed that ICPD 25 Nairobi Summit is a mutilation of the Cairo consensus . Its not acceptable guide to human development . @HonChrisWamalwa @WilliamsRuto @Pontifex @EWTNews https://t.co/kBtTJkMc1q\n",
      "BREAKING NEWS : The Nairobi Pro-life and Pro-family Declaration has been officially signed and adopted . @USAmbKenya @ntvkenya @KTNNewsKE @CatholicsKenya @NationBreaking @citizentvkenya @KBCChannel1 @InfoKfcb @cnnbrk @BBCBreaking @K24Tv @EbruTVKenya @HopeTvKenya @switchtvkenya https://t.co/hGNcmOpTw0\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Susan Wanjiru (@swanjirul)\n",
      "Ubicaci√≥n:  Kenya\n",
      "Descripci√≥n:  Eternity Calls\n",
      "325505312\n",
      "\n",
      "Tweets:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kenya is a GOD fearing nation , why are we now propagating laws on abortion , homosexuality . Have we stopped fearing GOD ? I am a sad Kenyan today #LetterToTheState\n",
      "Today I ask myself , what is of more value to this nation . Money or the sanctity of life ? Let us not destroy the future of Kenya with the ICPD Summit.Say no to abortion #LetterToTheState\n",
      "The Bible is Clear , Murder is sin and those who practise it they have their place in the eternal Hell fire ! I am asking today , why then legalize abortion ? Aren't you aware the Bible warns against killing ? #LetterToTheState\n",
      "I am not ashamed of the gospel , because it is the power of God for the salvation of everyone who believes : This gospel says NO TO SIN , , , Homosexuality is SinAbortion is Sin #LetterToTheState https://t.co/ptPx5btRrl\n",
      "Dear Kenyans , This is our country , and we are a GOD fearing people.JESUS paid a very heavy price for our redemption and salvation.Abortion is murder , we cannot allow it . #LetterToTheStatehttps :/ / t.co/xRJE5TSb5w\n",
      "Sin Is Sin and it separates a people and a nation form God ! Never can a state sit and Discuss an Agenda promoting Homosexuality and Abortion ! No ! These we reject ! #LetterToTheState https://t.co/zgion2Bz59\n",
      "#LetterToTheStateLet me ask you If abortion was legal at a time when our mothers gave birth to us , would we be born today ? Who would be the president of the country , the doctor or the teacher ? I say no to abortions and lesbianism\n",
      "As God fearing Nation üá∞ üá™ üá∞ üá™ , Kenyans we Reject the Pro-Abortion and Sexualization Agenda at ICPD + 25 Nairobi Summit . We can't allow evil to thrive in this Blessed Land . #LetterToTheState https://t.co/8ZcGgd7Erz\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "James Muchiri (@muchaz45)\n",
      "Ubicaci√≥n:  Nairobi, Kenya\n",
      "Descripci√≥n:  Engineer. Energy Practitioner. Reader. Writer. ....\n",
      "576426925\n",
      "\n",
      "Tweets:\n",
      "@sidchat1 @wanjiku_kaniaru @StateHouseKenya @Atayeshe Sir Kenyans have concerns with the ICPD 25 . Is abortion on demand an agenda ; homosexuality and gender ideology ? Kindly clarify if you've nothing to hide\n",
      "Loving my shirt . #ICPD25DOESNOTREPRESENTME as we show the unplanned Movie today marking the beginning of pro-life side events . #IMarchForLife @NairobiSummit @UNFPA https://t.co/2OJ3pYEiNV\n",
      "@FelistaWangari You call this propaganda ? I'm sorry , we know better ! Global Declaration on Abortion ; url = https://t.co/gjsw3jXqUy\n",
      "@doginde So sad watching this . I think our President has not shown enough leadership on this front . He recently attended the pro-abortion women deliver in Canada and today he's hanging out with these malevolent forces .\n",
      "@daktari1 @UNFPA @Atayeshe Knowledge not ignorance indeed ! Who's ignorant , we or you ? Global Declaration on Abortion ; url = https://t.co/gjsw3jXqUy\n",
      "@daktari1 @UNFPA @Atayeshe It's a shame and a mockery to God that you call yourself a servant of God , yet you support such a human right violation as mass killing of babies in the name of abortion . God is not mocked !\n",
      "History will not be kind to Uhuru1.He failed to unite us2.He wrecked an economy3.He sold us to China through debt4.He sought to keep power through BBI5.He is opening Kenya to abortion on demand6.He used DCI , DPP & KRA to target his opponents #LetterToTheState #NairobiSummit\n",
      "@UNCTADKituyi @AminaJMohammed @UNFPAKen Sir you're now also in support of this diabolical abortion and homosexuality agenda being pushed by @UNFPAKen ? Sane Kenyans have said NO to it !\n",
      "@StateHouseKenya History will be very unkind to Pres Kenyatta . He had a golden opportunity to reject the introduction of abortion agenda in Kenya but like the godless Esau he sold His birthright and that of Kenyans for a bowl of soup .\n",
      "@VP @realDonaldTrump Meanwhile in Kenya , Western governments mostly Scandinavian an @UN bodies like @UNFPAKen are holding a conference on abortion and homosexuality . Defund these bodies or disband them if they've nothing important to do\n",
      "@nc_missy @njokingumi Do I look ignorant ? Global Declaration on Abortion ; url = https://t.co/gjsw3jXqUy I'm well informed , Missy .\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "karimi (@Its_Karimi)\n",
      "Ubicaci√≥n:  None\n",
      "Descripci√≥n:  surrendered to God| Disciple of Christ|Creative|Farm Gal|planner|\n",
      "593512654\n",
      "\n",
      "Tweets:\n",
      "Sin Is Sin and it separates a people and a nation form God ! Never can a state sit and Discuss an Agenda promoting Homosexuality and Abortion ! No ! These we reject ! #LetterToTheState https://t.co/zgion2Bz59\n",
      "We as Kenyans are totally outraged at the glorification of homosexuality and abortion by the @unon and western powers ! we refuse and reject this campaign because our moral standards dont allow this barbaric acts that are against our beliefs and culture . #LetterToTheState\n",
      "The Bible is Clear , Murder is sin and those who practise it they have their place in the eternal Hell fire ! I am asking today , why then legalize abortion ? Aren't you aware the Bible warns against killing ? #LetterToTheState\n",
      "Homosexuality is sin , abortion is sin ! The bible is very clear clear that no homosexual or murderer will see the Kingdom of God ! Where will we be heading as a country if we legalize such vices . #LetterToTheState\n",
      "As God fearing Kenyans , we Reject the Pro-Abortion and Sexualization Agenda at ICPD + 25 Nairobi Summit . We can't allow evil to thrive in this Blessed Land . #LetterToTheState\n",
      "\" I have noticed that everyone who is for abortion has already been born . \" - Ronald Reagan #LetterToTheState\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "BAQTASH ABOUD AKASHA (@aboud_akasha)\n",
      "Ubicaci√≥n:  kenya\n",
      "Descripci√≥n:  blah blah staki kuskia ...\n",
      "598573081\n",
      "\n",
      "Tweets:\n",
      "#LetterToTheStateLet me ask you If abortion was legal at a time when our mothers gave birth to us , would we be born today ? Who would be the president of the country , the doctor or the teacher ? I say no to abortions and lesbianism\n",
      "Sin Is Sin and it separates a people and a nation form God ! Never can a state sit and Discuss an Agenda promoting Homosexuality and Abortion ! No ! These we reject ! #LetterToTheState https://t.co/zgion2Bz59\n",
      "The Bible is Clear , Murder is sin and those who practise it they have their place in the eternal Hell fire ! I am asking today , why then legalize abortion ? Aren't you aware the Bible warns against killing ? #LetterToTheState\n",
      "As God fearing Kenyans , we Reject the Pro-Abortion and Sexualization Agenda at ICPD + 25 Nairobi Summit . We can't allow evil to thrive in this Blessed Land . #LetterToTheState\n",
      "How wicked for our nation to legalize abortion and homosexuality . This is sin and God cannot condone such an evil thing . Let this #LetterToTheState reach president Kenyatta and take an action https://t.co/CPro82DVmx\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Wuod Japuonj (@OsumbaJM)\n",
      "Ubicaci√≥n:  Nairobi, Kenya\n",
      "Descripci√≥n:  http://kenyalaw.org\n",
      "\n",
      "ALL Opinions, MINE.\n",
      "618262601\n",
      "\n",
      "Tweets:\n",
      "No RATIONAL being can PUBLICLY advocate for HOMOSEXUALITY and / or ABORTION . If they do , then it only points to how degenerated such people areThe acts are UGLY , CRUEL & excessively SATANICAs for Human Rights , we refuse those that tell us to be GAYS #LetterToTheState\n",
      "No RATIONAL being can PUBLICLY advocate for HOMOSEXUALITY and / or ABORTION . If they do , then it only points to how degenerated such people areThe acts are UGLY , CRUEL & excessively SATANICAs for Human Rights , we refuse those that tell us to be GAYS #LetterToTheState\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "M.I.N.A.N.A (@ShinjitsuMeiyo)\n",
      "Ubicaci√≥n:  None\n",
      "Descripci√≥n:  None\n",
      "793245691\n",
      "\n",
      "Tweets:\n",
      "Sin Is Sin and it separates a people and a nation form God ! Never can a state sit and Discuss an Agenda promoting Homosexuality and Abortion ! No ! These we reject ! #LetterToTheState https://t.co/zgion2Bz59\n",
      "History will not be kind to Uhuru1.He failed to unite us2.He wrecked an economy3.He sold us to China through debt4.He sought to keep power through BBI5.He is opening Kenya to abortion on demand6.He used DCI , DPP & KRA to target his opponents #LetterToTheState #NairobiSummit\n",
      "We as Kenyans are totally outraged at the glorification of homosexuality and abortion by the @unon and western powers ! we refuse and reject this campaign because our moral standards dont allow this barbaric acts that are against our beliefs and culture . #LetterToTheState\n",
      "#LetterToTheState @StateHouseKenya Dear Mr President . The mess you have created for this country is enough . It is painful that you cannot understand why Kenyans are broke . We cannot be a nation separated from God by legalisation of abortion and gayism . You have done enough .\n",
      "All those who are advocating for abortion are aliveAll those who are advocating for homosexuality are borne of a man and a woman #LetterToTheState\n",
      "Dear Mr Presisent , This is a #LetterToTheState from sanity , moral and the born again Christian's of this precious county.Legalizing homosexuality and abortion is like taking the whole nation to hell.JESUS IS LORD IN THIS COUNTRY KENYA ! ! https://t.co/eddeD94Wsu\n",
      "\" We don ‚Äô t want the word to go out that we want to exterminate the Negro population . \" - Margaret Sanger , founder of Planned Parenthood , in her letter to Dr . Clarence J . Gamble , December 10 , 1939 . #LetterToTheState #NairobiSummit\n",
      "#LetterToTheState Let's not pretend that abortion isn't about the mass slaughter of innocents . I say No ! to Arbortion @UhuruKE @DonaldTrump\n",
      "#LetterToTheStateA beautiful nation like Kenya can't be spoiled by the evil of foreign agencies .. Abortion , Homosequality , Sexualization among the teenagers , Lesbianism ... This should not even be a debate .. It's just out of the Table.Say No ! ! https://t.co/dg8oiEQjp9\n",
      "We should note that abortion is Murder . Lesbianism and homosexuality is a grievous Sin #LetterToTheState\n",
      "\" I have noticed that everyone who is for abortion has already been born . \" - Ronald Reagan #LetterToTheState\n",
      "#LetterToTheState legalizing abortion leads to more abortion . We do not want to invoke God's wrath because of taking the lives of innocents\n",
      "****************************************\n",
      "****************************************\n",
      "****************************************\n",
      "****************************************\n",
      "\n",
      "Partici√≥n 3\n",
      "Cantidad de usuarios: 5575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tweets: 49824\n",
      "\n",
      "\n",
      "Usuarios m√°s RT \n",
      "\n",
      "\n",
      "1 . NARAL                  (@NARAL          ) -- 1469   RT\n",
      "2 . EMILY's List           (@emilyslist     ) -- 964    RT\n",
      "3 . Senator Mazie H        (@maziehirono    ) -- 876    RT\n",
      "4 . Lauren Rankin          (@laurenarankin  ) -- 760    RT\n",
      "5 . Rewire.News            (@Rewire_News    ) -- 583    RT\n",
      "6 . Janis Irwin            (@JanisIrwin     ) -- 470    RT\n",
      "7 . Center for Repr        (@ReproRights    ) -- 412    RT\n",
      "8 . Planned Parenth        (@PPact          ) -- 394    RT\n",
      "9 . Planned Parenth        (@PPFA           ) -- 344    RT\n",
      "10. UNFPA                  (@UNFPA          ) -- 280    RT\n",
      "11. Nairobi Summit         (@NairobiSummit  ) -- 274    RT\n",
      "12. The Unruly Squi        (@educatedadult1 ) -- 261    RT\n",
      "13. Leah Torres, MD        (@LeahNTorres    ) -- 250    RT\n",
      "14. prochoiceforall        (@prochoiceforal1) -- 236    RT\n",
      "15. robin heide            (@robinandriver  ) -- 227    RT\n",
      "16. John Pavlovitz         (@johnpavlovitz  ) -- 217    RT\n",
      "17. Renee Bracey Sh        (@RBraceySherman ) -- 214    RT\n",
      "18. ùôπùöûùöïùöíùöé                  (@resisterhood   ) -- 202    RT\n",
      "19. NARAL Pro-Choic        (@NARALVirginia  ) -- 192    RT\n",
      "20. Kamala Harris          (@KamalaHarris   ) -- 182    RT\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Dave Pell (@davepell)\n",
      "Ubicaci√≥n:  San Francisco\n",
      "Descripci√≥n:  Managing Editor, Internet.\n",
      "224\n",
      "\n",
      "Tweets:\n",
      "Pence is the one character whose story arc makes sense . He ‚Äô s a rabid religious nut who wants to outlaw abortion and sees gay people as the enemy . So he sells his soul to move those policies forward.It ‚Äô s all the other enablers I don ‚Äô t get .\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Steve Rhodes (@tigerbeat)\n",
      "Ubicaci√≥n:  San Francisco\n",
      "Descripci√≥n:  Photojournalist Also @tigerbeat @instagram Don't embed my photos as tweets/Instagram in stories Pay for my work DM or srhodes gmail Signal available on request\n",
      "29283\n",
      "\n",
      "Tweets:\n",
      "‚Äú We cannot accept references to ‚Äò sexual and reproductive health , ‚Äô nor any references to ‚Äò safe termination of pregnancy ‚Äô or language that would promote #abortion or suggest a right to abortion , ‚Äù ‚Å¶ @USAmbUN ‚Å© Kelly Craft told the #UN Security Council . https://t.co/zVCYlAd6G9\n",
      "Here's a primer on the most unqualified and ideologically extreme people Trump has confirmed to be lifetime federal judges.Let ' s just say it was hard to narrow it down . https://t.co/mTxyEkWC7G\n",
      "The GOP ‚Äô s fearmongering over abortion flopped in Kentucky and Virginia . Democrats need to get over their fear of the issue . https://t.co/RJNRwPpm2T by @aidachavez\n",
      "This denunciation of anti-Semitism & other bigotry feels gauchely performative & deeply hypocritical coming from someone who served Trump , defended his refugee family separation policy & opposed the choice of a well-regarded official for a UN post solely because he is Palestinian https://t.co/NHpcJ4eXzY\n",
      "LIVE : Reproductive rights legislative champions , physicians and patient storytellers speaking out to protect access to birth control . #ProtectX #SaveTitleX https://t.co/6JSpXPYX47\n",
      "Brett Kavanaugh is an imminent threat to reproductive rights and health care access . But that doesn ‚Äô t seem to bother FACEBOOK : They ‚Äô re sponsoring a dinner tonight where he ‚Äô s the keynote speaker . https://t.co/d3XGZcOmza\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Anil Dash ü•≠ (@anildash)\n",
      "Ubicaci√≥n:  NYC\n",
      "Descripci√≥n:  Helping everyone create the web together at üéè@Glitch. Host of @podcastfunction. üíú ethical tech, mangoes, funk, transit & justice. he/him\n",
      "36823\n",
      "\n",
      "Tweets:\n",
      "I'm all for Republicans learning the hard way that misogyny as a platform is a losing strategy . https://t.co/iovDvQf2dQ\n",
      "One thing that's clear from last night ? No one wants to see abortion banned in their state , including #Kentucky . 2/3 of KY voters say they want women to have access to abortion , one of many reasons #Bevin lost last night . https://t.co/Ql2CJ6wts2\n",
      "And I ‚Äô ve particularly appreciated , for example , when Melinda Gates has clearly & publicly talked about empowering women , including by ensuring access to abortion care . In the context of discussing presidential candidates , it makes obvious they must object to Trump . So say it .\n",
      "More Companies Are Openly Supporting Abortion Rights https://t.co/TsRlJoe78p\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Dam√Øe√± M√∫lley ¬∏ (@damienmulley)\n",
      "Ubicaci√≥n:  Cork, Ireland\n",
      "Descripci√≥n:  On a sabbatical type thing. Tweets mostly automated. Replies will be slow. No is my default. I like words that start with C. Reculer pour mieux sauter. He/Him.\n",
      "193753\n",
      "\n",
      "Tweets:\n",
      "Honoured , thrilled ( and a wee bit nervous ) to be in conversation with the incomparable ‚Å¶ @MargaretAtwood ‚Å© at @TheHist at ‚Å¶ @tcddublin ‚Å© tomorrow . #democracy #HumanRights #ProChoice #Feminism https://t.co/PpsRXyJKSS\n",
      "This afternoon @PresidentIRL reminded us that women ‚Äô s reproductive rights did not just arrive . There were decades of intimidation . There were difficult times . He thanked the women and men who took the @IrishFPA through the past 50 years : ‚Äò the vanguard ‚Äô . #SRHR #IFPA https://t.co/83CsWz3pmp\n",
      "\" As every advance in reproductive rights and health in Ireland was achieved , the Irish Family Planning Association was at the vanguard . \" President Higgins speaking about the @IrishFPA this afternoon : https://t.co/dlD7FfTkr6\n",
      "Here we go again \" Disability advocacy group condemns Govt proposal to introduce abortion for Down ‚Äô s syndrome to NI , likely up to birth \"\n",
      "Kentucky , which Trump won by 30 , looks like it just elected a Democrat who ran on expanding protections of the Affordable Care Act . The INCUMBENT Republican Governor he defeated pivoted hard at the end to make the race about abortion . The 2020 GOP playbook ain't looking good .\n",
      "Repeal wasn't won , the campaign ended . Prematurely and leaving many behind . Am too tired to celebrate book launches or take victory laps - too busy taking calls from people unable to access abortion care in Ireland . https://t.co/musgjqRyxb\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Wanda (@itsWanda)\n",
      "Ubicaci√≥n:  left of Main Street\n",
      "Descripci√≥n:  High-volume political account. #realSF #eatyourcolors #teamWarren #DubNation #SFGiants @dimsumeaters It's a goddamn beautiful day.\n",
      "483223\n",
      "\n",
      "Tweets:\n",
      "Everyone needs to know that we do not use D & E procedures just for surgical abortion . Many people choose this to manage their stillbirth in the 2nd trimester . Also , with chorioamnionitis before 24 weeks this can be a lifesaving procedure . https://t.co/YWn2Jike7O\n",
      "Oklahoma Supreme Court Blocks Ban on Common Abortion Method https://t.co/u5sXcmNhIk @CNSDallas https://t.co/vIjIwqFLT8\n",
      ". @realDonaldTrump has nominated some terrible people to our federal courts . Lawrence VanDyke is among the worst of the worst . He's attacked civil / human rights , opposed reproductive rights , and undermined gun safety regulations , and more . He doesn't belong on the 9th Circuit . https://t.co/kUUO35kdbJ\n",
      "THIS . THIS . A THOUSAND TIMES THIS . https://t.co/r4oZNt4pJV\n",
      "Reproductive health care decisions should be made between a patient and her doctor . I hope you ‚Äô ll join us in supporting the ROE Act today because anything medical should never be political . #roeact https://t.co/Xt4if4pzhu\n",
      "I mean , why can't we have both ? Promote pregnancy accommodations AND abortion access . Otherwise it really is coercion . \" Pro-life campaign threatens lawsuits against colleges for ‚Äò coercing students to abort ‚Äô \" https://t.co/gZTz1CD9nF via @collegefix\n",
      "Don ‚Äô t ask why Dems don ‚Äô t want to make room for an anti-abortion politician . Ask why the GOP doesn't want to make room for an anti-abortion politician who believes in climate change or immigration rights .\n",
      "This isn't a moral conviction . He has no morals . This is distain for women and a driving desire to suck up to the so-call Evangelical Christians . https://t.co/5n0hBOQ5wT\n",
      "We absolutely need to defeat Mitch McConnell in 2020 . But let's not anoint a KY Senate challenger prematurely . Amy McGrath had to walk back remarks on Kavanaugh . And she's not unequivocally pro-choice . Let KY voters decide and keep the pressure on . https://t.co/06FIzhSmvS\n",
      "Pro-choice champions flipped the Senate in VA . A pro-choice governor ‚Äô s been elected in KY . Research told us Reproductive Freedom & getting gov ‚Äô t out of women ‚Äô s personal decisions was a vote driver . Now we have the proof . The days of duck-and-cover on abortion are over .\n",
      "@GovMikeParson @DocMcnick @PPSLR @aclu_mo @PPFA @ACLU @GovParsonMO - Reproductive Health Services of @PPSLR is the only health center providing safe , legal abortion in the e n t i r e s t a t e of Missouri . #ShowMeAccess #MoLeg #BansOffMyBody https://t.co/ShH7zaTpCm\n",
      "Thank you to Marcela Howell at @BlackWomensRJ for reminding us that a right is * not * a right if it's inaccessible because of barriers such as cost , distance to the nearest clinic that provides abortion care , and discrimination . #ShowMeAccess https://t.co/qwZWCDRY5O\n",
      "Decisions about your birth control should involve : ‚úÖ You ‚úÖ Your healthcare provider , if you choose ‚ùå Your boss ‚ùå Anti-choice politicians #ThxBirthControl\n",
      "Fake women ‚Äô s health centers ‚Äî bankrolled by religious orgs , anti-choice billionaires , & OUR TAX DOLLARS ‚Äî are moving into our neighborhoods to shame healthy sexual behavior and spread flat-out lies about abortion and hormonal birth control . https://t.co/A1LXR9iKqc #ExposeFakeClinics\n",
      "Fun fact : Ascension , the healthcare network that Google is partnering with for this massive data harvesting project , is a network of Catholic hospitals that are vehemently anti-choice https://t.co/QchTodq2Oi https://t.co/r4AGKqcIcp\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "just adrienne for now (@adrienneleigh)\n",
      "Ubicaci√≥n:  \n",
      "Descripci√≥n:  One step ahead of the shoeshine, two steps away from the county line. \n",
      "@Doc_Destructo's flower. Cis-ish; she/her.\n",
      "557563\n",
      "\n",
      "Tweets:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There ‚Äô s a reason Shelby County , which gutted voting rights was decided two years before Obergefell ‚Äî and we heard minimal repulse from LGBTQ organizations . Similar reasons they are silent around abortion , contraception , sex work , among others . We ‚Äô re asterisks .\n",
      "This week , the U . S . Ambassador to the United Nations objected to the use of the phrase \" sexual and reproductive health \" because it could \" suggest a right to abortion . \" Make no mistake : They are coming for women's freedom on a global scale . https://t.co/1BFBnS2ppf\n",
      "@gaywonk @alexkotch @ebruenig No thank you . Anti-abortion \" leftists \" need to gtfo .\n",
      "I'm done with Democratic strategists worrying that supporting abortion rights will cause a candidate to lose . 73 % of Americans want Roe v . Wade to remain the law of the land . But more importantly , abortion is a fundamental right . We don't discard human rights to win elections .\n",
      "Refusing to support abortion as a candidate only allows abortion stigma to flourish . Then it becomes a self-fulfilling prophecy : of course it ‚Äô s a losing issue . No one has embraced it . It SEEMS stigmatized and shameful . It reinforces a culture of silence and fear around abortion .\n",
      "Hello these are truths I believe feel free to unfollow if they make you uncomfortable . - don ‚Äô t hit children . - sex work is work . - abortion is a right . - gender isn ‚Äô t binary . - yt ‚Äô s uphold systems of racism even if subconsciously . - some other races do too . - my ass is great .\n",
      "A California woman has been charged with first-degree murder and has a bail of $ 5 million for giving birth to a stillborn . Pregnant people are already being criminalized for their pregnancy outcomes IN PRO-CHOICE STATES . https://t.co/yMDKRz4P0m\n",
      "@rozietoez @JuliaMasonMD1 @gritmonger @ysabel @arthur_affect @remembrancermx @BrynnTannehill @jack_turban @KWierso It's fascinating how the framing differs . A 95 % \" don't regret \" rate with abortion is \" hardly anyone regrets \" , but a 0.4 % transition regret rate is way too many to contemplate.https :/ / t.co/iXoheUe0tC\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "CNN (@CNN)\n",
      "Ubicaci√≥n:  None\n",
      "Descripci√≥n:  It‚Äôs our job to #GoThere & tell the most difficult stories. Join us! For more breaking news updates follow @CNNBRK  & Download our app http://cnn.com/apps\n",
      "759251\n",
      "\n",
      "Tweets:\n",
      "A federal judge in New York has blocked HHS ' so-called conscience rule , which lets health care workers who cite moral or religious reasons opt out of providing certain medical procedures , such as abortion , sterilization and assisted suicide https://t.co/hYzg1SCUCj\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Liza Sabater üáµüá∑üë∏üèæüåπ (@blogdiva)\n",
      "Ubicaci√≥n:  in the recesses of your brain\n",
      "Descripci√≥n:  free-range professor & hacker. proud afroboricua. antifa, socialista & #PRExit'ista. shmears arts, history, philosophy, politics & tech with baby hippos ü¶õ\n",
      "820694\n",
      "\n",
      "Tweets:\n",
      "\" Our bodies , our choices \" does not just apply to abortion procedures . It applies to many , including intersex & trans people . Policing medical procedures is beyond dangerous . \" Society norms \" is not a reason to hurt members of our society . https://t.co/ImXBTeVH0V\n",
      "LOUDER FOR THE CHICKENSHITS IN THE BACK https://t.co/7fgNR11JjK\n",
      "@badgaIcari @daniecal @yeloson yup . and the MDs are proxy rapists . think of the ultrasounds forced on women wanting an abortion . those are state mandated rape by medical exam . like they did to thousands during Operation Condor in Latin America.and what gringos did in Iraq ‚Äô s AbuGhraib\n",
      "ImmigrationPolicingHousing & homelessnessLeadIndigenous issuesEducationLGBTQ reproductive rightsThese are issues @JulianCastro have brought to the debate stage that will be lost without him . https://t.co/TLqzBN5ueZ\n",
      "You understand that as long as Trump is nominating savage idiots to the judiciary who will uphold abortion restrictions and overturn Roe , no one on the right will give a shit about how fucking crazy and incompetent and dangerous he is .\n",
      "Planned Parenthood in Missouri has endorsed @nicolergalloway for governor . She's facing @mikeparson , who signed a bill criminalizing abortion after 8 weeks . @J_Hancock recently wrote about how abortion will play a big role in the race.https :/ / t.co/uMmkx7Miwk #mogov #moleg\n",
      "The 40 year plan to get rid of #RoeVWade is upon us . In this Boom ! Lawyered , @hegemommy and @angryblacklady explain the arguments that would fuel a legal challenge and how that decision could be used to attack state laws protecting abortion rights.https :/ / t.co/UEblxg6yfr\n",
      "RAMPANT SEXISMThe governments of fascist nations tend to be almost exclusively male-dominated ... traditional gender roles are made more rigid . Divorce , abortion and homosexuality are suppressed and the state is represented as the ultimate guardian of the family institution . https://t.co/ygu0Iij5HO\n",
      "We ‚Äô re recruiting pro-choice Democratic women of all backgrounds to run for office in 2019 , 2020 and beyond . Sound like you ? Sign up to learn more :\n",
      "Pro-choice champions flipped the Senate in VA . A pro-choice governor ‚Äô s been elected in KY . Research told us Reproductive Freedom & getting gov ‚Äô t out of women ‚Äô s personal decisions was a vote driver . Now we have the proof . The days of duck-and-cover on abortion are over .\n",
      "As part of flipping the Virginia General Assembly and another victory for prochoice advocates , Democrat Suhas Subramanyam defeated the GOP candidate who suggested ankle bracelets to monitor pregnant women to prevent abortion . #ElectionResults2019 #BlueWave https://t.co/xaVC2dnsY6\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Amanda Marcotte (@AmandaMarcotte)\n",
      "Ubicaci√≥n:  \n",
      "Descripci√≥n:  Politics writer for https://t.co/qNTcMZjdvo. Author of Troll Nation. https://t.co/Anr0bCFB2h\n",
      "928481\n",
      "\n",
      "Tweets:\n",
      "Oxytocin is also the hormone released when parents bond with children . So , by their own logic , anti-choicers should believe women should have no more than one child , as they are incapable of loving any after the first . https://t.co/tziS4Ayy8S https://t.co/siMOyA7Sem\n",
      "This guy ‚Äô s explanation makes no sense and I think it ‚Äô s just that anti-choice nuts are often sublimating sexual fantasies into weird political acts . https://t.co/oonqfE8R6m\n",
      "Particularly critical because , in the upcoming fight over the ERA , conservatives will lean heavily on accusing women of being baby-killers . And that rhetoric is ... not popular .\n",
      "This kind of framing , from Caitlyn Flanagan ‚Äô s new effort to stigmatize abortion , sounds all very noble and rational . Who doesn ‚Äô t want to think of themselves as someone dispassionately considering arguments strictly by their merits ? https://t.co/2oVUE3BrMf\n",
      "To truly understand the abortion debate , we have to look at the arguments that actually motivate antis . And that argument is that women shouldn ‚Äô t be able to have sex only for pleasure . https://t.co/NsKbfM7wqR\n",
      "But some of the people that are pro-choice are also supportive of * racist * policies ( stop-and-frisk , school segregation ) that are very much not liberal . But those folks , while overrepresented in NYC , aren't a big group outside of areas like that .\n",
      "I'll add , in response to some comments , that the \" socially liberal \" designation is also troubling , since it tends to be measured along what I'd call patriarchy support ‚Äî if you're pro-choice and pro-same-sex marriage , you get called \" liberal \" .\n",
      "The problem , however , is that arguments don ‚Äô t exist independently of their purpose . And discouraging people to consider the source of anti-choice arguments is about hiding extremely important contextual information : They aren ‚Äô t motivated by ‚Äú life ‚Äù or ‚Äú best arguments ‚Äù .\n",
      "==================================================\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "Brooke Binkowski (@brooklynmarie)\n",
      "Ubicaci√≥n:  None\n",
      "Descripci√≥n:  Chaotic good lovable rogue. Retweets mean I'm in front of your house in an unmarked vehicle. Managing editor @erumors.\n",
      "1044241\n",
      "\n",
      "Tweets:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is it with these men ? Is it a fetish ? Missouri health director kept spreadsheet of Planned Parenthood patients ‚Äô periods https://t.co/NeyBzTvbvW\n",
      "Remember when Adriana Cohen on @RealClearNews said ‚Äú It's well-documented that many conservative leaders & activists have been unwelcome or outright banned from speaking at colleges and universities , including pro-life & conservative commentator Ben Shapiro . ‚Äù Are they ok w this üëá https://t.co/VIc1NQJxCg\n",
      "There ‚Äô s literally a whole fucking party dedicated to anti-abortion hardlining , he can join that one , christ allflippingmighty , why would anyone even ask this https://t.co/Q9ArLUeWV9\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_partitions):\n",
    "    community_info(i, num_users=10, followers=20, num_tweets=3, user_threshold=user_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Armamos datasets con los tweets de cada comunidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_of_community(partition_num):\n",
    "    users = list(users_of_community(db, partition_num))\n",
    "    user_ids = [user[\"id\"] for user in users]\n",
    "    tweets = [tweet['tweet']['full_text'] for tweet in db.abortion.find({**base_query,**{\"user\": {\"$in\": user_ids}}}, {\"tweet.full_text\": 1})]\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets_of_community' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6332205779ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Favor y against are merely orientative names. At this point it's not important which stance takes the community, we are going to decide that later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtweets_favor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets_of_community\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtweets_agains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets_of_community\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweets_of_community' is not defined"
     ]
    }
   ],
   "source": [
    "# Favor y against are merely orientative names. At this point it's not important which stance takes the community, we are going to decide that later\n",
    "# Nevertheless, I checked the community info and assigned each community to the name I though was a better fit, though we are going to validate this in the future\n",
    "tweets_favor = tweets_of_community(3)\n",
    "tweets_agains = tweets_of_community(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_train = open(\"abortion_favor_dataset_train.tsv\", 'w')\n",
    "w_train.write(\"text\\n\")\n",
    "for tweet in tweets_favor[:-200]:\n",
    "    w_train.write(\"{}\\n\".format(tweet.replace('\\n','').replace('\\t','')))\n",
    "w_train.close()\n",
    "\n",
    "w_val = open(\"abortion_favor_dataset_val.tsv\", 'w')\n",
    "w_val.write(\"text\\n\")\n",
    "for tweet in tweets_favor[-200:-100]:\n",
    "    w_val.write(\"{}\\n\".format(tweet.replace('\\n','').replace('\\t','')))\n",
    "w_val.close()\n",
    "\n",
    "w_test = open(\"abortion_favor_dataset_test.tsv\", 'w')\n",
    "w_test.write(\"text\\n\")\n",
    "for tweet in tweets_favor[-100:]:\n",
    "    w_test.write(\"{}\\n\".format(tweet.replace('\\n','').replace('\\t','')))\n",
    "w_test.close()\n",
    "\n",
    "w_train_against = open(\"abortion_against_dataset_train.tsv\", 'w')\n",
    "w_train_against.write(\"text\\n\")\n",
    "for tweet in tweets_agains[:-200]:\n",
    "    w_train_against.write(\"{}\\n\".format(tweet.replace('\\n','').replace('\\t','')))\n",
    "w_train_against.close()\n",
    "\n",
    "w_val_against = open(\"abortion_against_dataset_val.tsv\", 'w')\n",
    "w_val_against.write(\"text\\n\")\n",
    "for tweet in tweets_agains[-200:-100]:\n",
    "    w_val_against.write(\"{}\\n\".format(tweet.replace('\\n','').replace('\\t','')))\n",
    "w_val_against.close()\n",
    "\n",
    "w_test_against = open(\"abortion_against_dataset_test.tsv\", 'w')\n",
    "w_test_against.write(\"text\\n\")\n",
    "for tweet in tweets_agains[-100:]:\n",
    "    w_test_against.write(\"{}\\n\".format(tweet.replace('\\n','').replace('\\t','')))\n",
    "w_test_against.close()\n",
    "\n",
    "w_all = open(\"abortion_complete_dataset.tsv\", 'w')\n",
    "w_all.write(\"text\\n\")\n",
    "for tweet in tweets_agains:\n",
    "    w_all.write(\"{}\\n\".format(tweet.replace('\\n','').replace('\\t','')))\n",
    "for tweet in tweets_favor:\n",
    "    w_all.write(\"{}\\n\".format(tweet.replace('\\n','').replace('\\t','')))\n",
    "w_all.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(\n",
    "    tokenizer_language='en',\n",
    "    lower=True,\n",
    "    init_token='<sos>',\n",
    "    eos_token='<eos>',\n",
    "    batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_favor, val_favor, test_favor = data.TabularDataset.splits(path=\"\", train=\"abortion_favor_dataset_train.tsv\", validation=\"abortion_favor_dataset_val.tsv\", test=\"abortion_favor_dataset_test.tsv\", format=\"tsv\", fields=[('text', TEXT)])\n",
    "train_against, val_against, test_against = data.TabularDataset.splits(path=\"\", train=\"abortion_against_dataset_train.tsv\", validation=\"abortion_against_dataset_val.tsv\", test=\"abortion_against_dataset_test.tsv\", format=\"tsv\", fields=[('text', TEXT)])\n",
    "train_all = data.TabularDataset(path=\"abortion_complete_dataset.tsv\", format=\"tsv\", fields=[(\"text\", TEXT)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 100087 tokens in our vocabulary\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_all, vectors=\"glove.6B.300d\")\n",
    "print(f\"We have {len(TEXT.vocab)} tokens in our vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset2example(dataset):\n",
    "    examples = list(map(lambda example: ['_bos_']+ example.text + ['_eos_'], dataset.examples))\n",
    "    examples = [item for example in examples for item in example]\n",
    "    example = data.Example()\n",
    "    setattr(example, 'text', examples)\n",
    "    return data.Dataset([example], fields={'text': TEXT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_favor = dataset2example(train_favor)\n",
    "val_favor = dataset2example(val_favor)\n",
    "test_favor = dataset2example(test_favor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_against = dataset2example(train_against)\n",
    "val_against = dataset2example(val_against)\n",
    "test_against = dataset2example(test_against)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We get the neutral data from other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0128 12:10:18.849649 140620505528064 file_utils.py:35] PyTorch version 1.4.0 available.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "import transformers\n",
    "import sklearn\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule_with_warmup\n",
    "\n",
    "from io import open\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "    printable = set(string.printable)\n",
    "    ans = []\n",
    "    for word in tweet.split(' '):\n",
    "        if not word.startswith(\"@\") and not word.startswith(\"http\"):\n",
    "            ans.append(''.join(list(filter(lambda x: x in printable, word))))\n",
    "    return ' '.join(ans).replace('\\n','').replace('\\t','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = open(\"feminism_community_based_classification.csv\", 'w')\n",
    "# w.write(\"text\\tlabel\\n\")\n",
    "\n",
    "# # favor_train_ds = open(\"abortion_favor_dataset_train.tsv\", 'r')\n",
    "# # favor_test_ds = open(\"abortion_favor_dataset_test.tsv\", 'r')\n",
    "# # favor_dev_ds = open(\"abortion_favor_dataset_val.tsv\", 'r')\n",
    "# # tweets_all_favor = set([preprocess(line) for line in favor_train_ds]).union([preprocess(line) for line in favor_test_ds]).union([preprocess(line) for line in favor_dev_ds])\n",
    "# favor_ds = open(\"feminism_favor_dataset.tsv\")\n",
    "# tweets_all_favor = set([preprocess(line) for line in favor_ds])\n",
    "# for tweet in tweets_all_favor:\n",
    "#     if tweet.replace(' ','') != \"\":\n",
    "#         w.write(\"{}\\tFAVOR\\n\".format(tweet))climate-change_community_based_classification\n",
    "\n",
    "# print(len(tweets_all_favor))\n",
    "\n",
    "# # against_train_ds = open(\"abortion_against_dataset_train.tsv\", 'r')\n",
    "# # against_test_ds = open(\"abortion_against_dataset_test.tsv\", 'r')\n",
    "# # against_dev_ds = open(\"abortion_against_dataset_val.tsv\", 'r')\n",
    "# # tweets_all_against = set([preprocess(line) for line in against_train_ds]).union([preprocess(line) for line in against_test_ds]).union([preprocess(line) for line in against_dev_ds])\n",
    "# against_ds = open(\"feminism_against_dataset.tsv\", 'r')\n",
    "# tweets_all_against = set([preprocess(line) for line in against_ds])\n",
    "# for tweet in tweets_all_against:\n",
    "#     if tweet.replace(' ','') != \"\":\n",
    "#         w.write(\"{}\\tAGAINST\\n\".format(tweet))\n",
    "\n",
    "# print(len(tweets_all_against))\n",
    "    \n",
    "# abortion = open(\"abortion_unlabeled_sample_complete_without_preprocessing.csv\", 'r')\n",
    "# feminism = open(\"climate-change_unlabeled_complete_without_preprocessing.csv\", 'r')\n",
    "# climate_list = set()\n",
    "# femin_list = set()\n",
    "# for idx, line in enumerate(abortion):\n",
    "#     climate_list.add(\"{}\\tNONE\\n\".format(preprocess(line)))\n",
    "# for idx, line in enumerate(feminism):\n",
    "#     femin_list.add(\"{}\\tNONE\\n\".format(preprocess(line)))\n",
    "\n",
    "# l = int((float(len(tweets_all_favor) + len(tweets_all_against)) * 0.2) / 0.8)\n",
    "# print(l)\n",
    "# for tweet in random.sample(climate_list.union(femin_list), l):\n",
    "#     w.write(tweet)\n",
    "\n",
    "\n",
    "# w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    \"bert\": (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "}\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for token classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, sentence, label):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            words: list. The words of the sequence.\n",
    "            label: (Optional) single value. The label for the sequence. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.sentence = sentence\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "n_gpu = 0\n",
    "\n",
    "# Set seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0128 12:10:28.295554 140620505528064 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at /home/dfurman/.cache/torch/transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.788fed32bb8481a9b15ce726d41c53d5d5066b04c667e34ce3a7a3826d1573d8\n",
      "I0128 12:10:28.297363 140620505528064 configuration_utils.py:199] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0128 12:10:28.959421 140620505528064 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at /home/dfurman/.cache/torch/transformers/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0128 12:10:29.693306 140620505528064 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /home/dfurman/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "I0128 12:10:36.925949 140620505528064 modeling_utils.py:480] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0128 12:10:36.926913 140620505528064 modeling_utils.py:483] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(n_gpu):\n",
    "    # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
    "    pad_token_label_id = CrossEntropyLoss().ignore_index\n",
    "\n",
    "    model_type = \"bert\"\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "\n",
    "    bert_type = \"bert-large-uncased\"\n",
    "    config = config_class.from_pretrained(bert_type,\n",
    "                                              num_labels=3)\n",
    "    tokenizer = tokenizer_class.from_pretrained(bert_type,\n",
    "                                                    do_lower_case=True)\n",
    "    model = model_class.from_pretrained(bert_type, from_tf=False,\n",
    "                                            config=config)\n",
    "    model.to(device)\n",
    "\n",
    "    'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples,\n",
    "                                 label_list,\n",
    "                                 max_seq_length,\n",
    "                                 tokenizer,\n",
    "                                 cls_token_at_end=False,\n",
    "                                 cls_token=\"[CLS]\",\n",
    "                                 cls_token_segment_id=1,\n",
    "                                 sep_token=\"[SEP]\",\n",
    "                                 sep_token_extra=False,\n",
    "                                 pad_on_left=False,\n",
    "                                 pad_token=0,\n",
    "                                 pad_token_segment_id=0,\n",
    "                                 pad_token_label_id=-1,\n",
    "                                 sequence_a_segment_id=0,\n",
    "                                 mask_padding_with_zero=True):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "#         if ex_index % 10000 == 0:\n",
    "#             print(\"Writing example {} of {}\".format(ex_index, len(examples)))\n",
    "#             print(\"E.g: {}\".format(example.words))\n",
    "\n",
    "        tokens = []\n",
    "        for word in example.sentence.split(' '):\n",
    "            # TODO: handle punctuation and preprocess (e.g. remove #SemST, @..., and http://)\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            tokens.extend(word_tokens)\n",
    "        \n",
    "\n",
    "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
    "        special_tokens_count = 3 if sep_token_extra else 2\n",
    "        if len(tokens) > max_seq_length - special_tokens_count:\n",
    "            tokens = tokens[:(max_seq_length - special_tokens_count)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids:   0   0   0   0  0     0   0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens += [sep_token]\n",
    "\n",
    "        if sep_token_extra:\n",
    "            # roberta uses an extra separator b/w pairs of sentences\n",
    "            tokens += [sep_token]\n",
    "\n",
    "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "        if cls_token_at_end:\n",
    "            tokens += [cls_token]\n",
    "            segment_ids += [cls_token_segment_id]\n",
    "        else:\n",
    "            tokens = [cls_token] + tokens\n",
    "            segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        if pad_on_left:\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\n",
    "            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
    "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "        else:\n",
    "            input_ids += ([pad_token] * padding_length)\n",
    "            input_mask += ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "            segment_ids += ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_map[example.label]))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a lot of fun hearing @SethAndrewsTTA talk about his new book #SacredCows. @ThinkingAtheist #GodlessGala #SemST\n",
      "FAVOR\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"semeval.atheism.train.csv\", sep=',', encoding=\"latin1\").fillna(method=\"ffill\")\n",
    "\n",
    "# getter = SentenceGetter(data)\n",
    "#Sacarle las oraciones que empiezan con \"<\", el autor, abstract, etc\n",
    "\n",
    "sentences = data['Tweet']\n",
    "labels = data['Stance']\n",
    "\n",
    "num_labels = len(labels)\n",
    "\n",
    "examples = [InputExample(guid, words, labels) for guid, (words, labels) in enumerate(zip(sentences, labels))]\n",
    "print(examples[22].sentence)\n",
    "print(examples[22].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.device(n_gpu):\n",
    "    possible_labels = [\"NONE\", \"FAVOR\", \"AGAINST\"]\n",
    "    features = convert_examples_to_features(examples, possible_labels, 50, tokenizer,\n",
    "                                                    cls_token_at_end=False,\n",
    "                                                    # xlnet has a cls token at the end\n",
    "                                                    cls_token=tokenizer.cls_token,\n",
    "                                                    cls_token_segment_id=0,\n",
    "                                                    sep_token=tokenizer.sep_token,\n",
    "                                                    sep_token_extra=False,\n",
    "                                                    # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n",
    "                                                    pad_on_left=False,\n",
    "                                                    # pad on the left for xlnet\n",
    "                                                    pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "                                                    pad_token_segment_id=0,\n",
    "                                                    pad_token_label_id=pad_token_label_id\n",
    "                                                    )\n",
    "\n",
    "\n",
    "        # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "\n",
    "    all_input_ids.to(device)\n",
    "    all_input_mask.to(device)\n",
    "    all_segment_ids.to(device)\n",
    "    all_label_ids.to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will never control yourself until you confront self as a problem.-Pastor Merritt #Christian #Church #SemST\n",
      "AGAINST\n"
     ]
    }
   ],
   "source": [
    "devdata = pd.read_csv(\"semeval.atheism.validation.csv\", sep=',', encoding=\"latin1\").fillna(method=\"ffill\")\n",
    "\n",
    "sentences = devdata['Tweet']\n",
    "labels = devdata['Stance']\n",
    "\n",
    "num_labels = len(labels)\n",
    "\n",
    "examples = [InputExample(guid, words, labels) for guid, (words, labels) in enumerate(zip(sentences, labels))]\n",
    "print(examples[22].sentence)\n",
    "print(examples[22].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.device(n_gpu):\n",
    "    possible_labels = [\"NONE\", \"FAVOR\", \"AGAINST\"]\n",
    "    features = convert_examples_to_features(examples, possible_labels, 50, tokenizer,\n",
    "                                                    cls_token_at_end=False,\n",
    "                                                    # xlnet has a cls token at the end\n",
    "                                                    cls_token=tokenizer.cls_token,\n",
    "                                                    cls_token_segment_id=0,\n",
    "                                                    sep_token=tokenizer.sep_token,\n",
    "                                                    sep_token_extra=False,\n",
    "                                                    # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n",
    "                                                    pad_on_left=False,\n",
    "                                                    # pad on the left for xlnet\n",
    "                                                    pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "                                                    pad_token_segment_id=0,\n",
    "                                                    pad_token_label_id=pad_token_label_id\n",
    "                                                    )\n",
    "\n",
    "\n",
    "        # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "\n",
    "    all_input_ids.to(device)\n",
    "    all_input_mask.to(device)\n",
    "    all_segment_ids.to(device)\n",
    "    all_label_ids.to(device)\n",
    "\n",
    "    dev_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    dev_sampler = RandomSampler(dev_dataset)\n",
    "    dev_dataloader = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He became a slave to the cross that men could go free! #SemST\n",
      "AGAINST\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testdata = pd.read_csv(\"semeval.atheism.test.csv\", sep=',', encoding=\"latin1\").fillna(method=\"ffill\")\n",
    "\n",
    "sentences = testdata['Tweet']\n",
    "labels = testdata['Stance']\n",
    "\n",
    "num_labels = len(labels)\n",
    "\n",
    "examples = [InputExample(guid, words, labels) for guid, (words, labels) in enumerate(zip(sentences, labels))]\n",
    "print(examples[22].sentence)\n",
    "print(examples[22].label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.device(n_gpu):\n",
    "    possible_labels = [\"NONE\", \"FAVOR\", \"AGAINST\"]\n",
    "    features = convert_examples_to_features(examples, possible_labels, 50, tokenizer,\n",
    "                                                    cls_token_at_end=False,\n",
    "                                                    # xlnet has a cls token at the end\n",
    "                                                    cls_token=tokenizer.cls_token,\n",
    "                                                    cls_token_segment_id=0,\n",
    "                                                    sep_token=tokenizer.sep_token,\n",
    "                                                    sep_token_extra=False,\n",
    "                                                    # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n",
    "                                                    pad_on_left=False,\n",
    "                                                    # pad on the left for xlnet\n",
    "                                                    pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "                                                    pad_token_segment_id=0,\n",
    "                                                    pad_token_label_id=pad_token_label_id\n",
    "                                                    )\n",
    "\n",
    "\n",
    "        # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "\n",
    "    all_input_ids.to(device)\n",
    "    all_input_mask.to(device)\n",
    "    all_segment_ids.to(device)\n",
    "    all_label_ids.to(device)\n",
    "\n",
    "    test_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    test_sampler = RandomSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, test_dataloader):\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    model.eval()\n",
    "    device = \"cuda\"\n",
    "    examples = []\n",
    "    with torch.cuda.device(n_gpu):\n",
    "        for batch in test_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                inputs = {\"input_ids\": batch[0],\n",
    "                          \"attention_mask\": batch[1],\n",
    "                          \"token_type_ids\": batch[2],\n",
    "                          # XLM and RoBERTa don\"t use segment_ids\n",
    "                          \"labels\": batch[3]}\n",
    "                outputs = model(**inputs)\n",
    "                tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "\n",
    "                eval_loss += tmp_eval_loss.item()\n",
    "            nb_eval_steps += 1\n",
    "            if preds is None:\n",
    "                preds = logits.detach().cpu().numpy()\n",
    "                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "            else:\n",
    "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "            examples.append(tokenizer.convert_ids_to_tokens([w for b in batch[0] for w in b.detach().cpu().numpy()]))\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "\n",
    "        return eval_loss, preds, out_label_ids, examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, model, tokenizer, labels, pad_token_label_id, dev_dataloader, test_dataloader, train_batch_size=16, num_train_epochs=8, scheduler_type=\"linear\", lr=1e-5):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    \n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8, correct_bias=False)\n",
    "    if scheduler_type == \"linear\":\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=len(train_dataloader) * num_train_epochs * 0.1, num_training_steps=len(train_dataloader)*num_train_epochs)\n",
    "    else:\n",
    "        scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=len(train_dataloader) * num_train_epochs * 0.1)#, num_training_steps=t_total)\n",
    "#     scheduler = WarmupLinearSchedule(optimizer, warmup_steps=120, t_total=t_total)\n",
    "\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    train_loss_timeline = []\n",
    "    eval_loss_timeline = []\n",
    "    f1_timeline_dev = []\n",
    "    f1_timeline_test = []\n",
    "    precision_timeline_test = []\n",
    "    precision_timeline_dev = []\n",
    "    recall_timeline_test = []\n",
    "    recall_timeline_dev = []\n",
    "    accuracy_timeline_test = []\n",
    "    accuracy_timeline_dev = []\n",
    "    \n",
    "    f1_timeline_dev_micro = []\n",
    "    f1_timeline_test_micro = []\n",
    "    precision_timeline_test_micro = []\n",
    "    precision_timeline_dev_micro = []\n",
    "    recall_timeline_test_micro = []\n",
    "    recall_timeline_dev_micro = []\n",
    "    \n",
    "    f1_timeline_dev_none = []\n",
    "    f1_timeline_test_none = []\n",
    "    precision_timeline_test_none = []\n",
    "    precision_timeline_dev_none = []\n",
    "    recall_timeline_test_none = []\n",
    "    recall_timeline_dev_none = []\n",
    "    \n",
    "    all_preds_dev = []\n",
    "    all_preds_test = []\n",
    "    \n",
    "    community_favor_accuracy_timeline = []\n",
    "    community_against_accuracy_timeline = []\n",
    "    \n",
    "    community_favor_preds_timeline = []\n",
    "    community_against_preds_timeline = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(num_train_epochs), desc=\"Epoch\", disable=False)\n",
    "    w = open(\"twitter-stance_supervised_atheism_{}_{}_{}_{}\".format(bert_type, scheduler_type, lr, batch_size), 'w')\n",
    "    w_preds = open(\"twitter-stance_supervised_atheism_{}_{}_{}_{}_preds\".format(bert_type, scheduler_type, lr, batch_size), 'w')\n",
    "    set_seed(42)  # Added here for reproductibility (even between python 2 and 3)\n",
    "    \n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=True)\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {\"input_ids\": batch[0],\n",
    "                        \"attention_mask\": batch[1],\n",
    "                        \"token_type_ids\": batch[2],\n",
    "                        # XLM and RoBERTa don\"t use segment_ids\n",
    "                        \"labels\": batch[3]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "            loss.backward()\n",
    "#             ipdb.set_trace()\n",
    "            tr_loss += loss.item()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            global_step += 1\n",
    "\n",
    "        # Evaluamos en dev\n",
    "        loss, preds, truth_dev, examples = evaluate(model, dev_dataloader)\n",
    "        eval_loss_timeline.append(loss)\n",
    "        train_loss_timeline.append(tr_loss / global_step)\n",
    "        \n",
    "        f1_timeline_dev.append(metrics.f1_score(preds, truth_dev, average=\"macro\"))\n",
    "#         precision_timeline_dev.append(metrics.precision_score(preds, truth_dev, average=\"macro\"))\n",
    "#         recall_timeline_dev.append(metrics.recall_score(preds,truth_dev, average=\"macro\"))\n",
    "        \n",
    "        f1_timeline_dev_micro.append(metrics.f1_score(preds, truth_dev, average=\"micro\"))\n",
    "#         precision_timeline_dev_micro.append(metrics.precision_score(preds, truth_dev, average=\"micro\"))\n",
    "#         recall_timeline_dev_micro.append(metrics.recall_score(preds,truth_dev, average=\"micro\"))\n",
    "        \n",
    "        ff11 = metrics.f1_score(preds, truth_dev, average=None)\n",
    "        f1_timeline_dev_none.append((ff11[1] + ff11[2]) / 2)\n",
    "        \n",
    "#         ppp = metrics.precision_score(preds, truth_dev, average=None)\n",
    "#         precision_timeline_dev_none.append((ppp[1] + ppp[2]) / 2)\n",
    "        \n",
    "#         rrr = metrics.recall_score(preds,truth_dev, average=None)\n",
    "#         recall_timeline_dev_none.append((rrr[1] + rrr[2]) / 2)\n",
    "        \n",
    "#         accuracy_timeline_dev.append(metrics.accuracy_score(preds, truth_dev))\n",
    "        \n",
    "        all_preds_dev.append(preds)\n",
    "        \n",
    "        # Evaluamos en test asi cuando sabemos el mejor epoch usando dev ya tenemos el resultado de test calculado\n",
    "        loss, preds, truth, examples = evaluate(model, test_dataloader)\n",
    "        \n",
    "#         preds_favor = []\n",
    "#         preds_against = []\n",
    "#         for p, t in zip(preds, truth):\n",
    "#             if t == 1:\n",
    "#                 preds_favor.append(p)\n",
    "#             if t == 2:\n",
    "#                 preds_against.append(p)\n",
    "        \n",
    "#         community_favor_preds_timeline.append(preds_favor)\n",
    "#         community_against_preds_timeline.append(preds_against)\n",
    "        \n",
    "#         f1_timeline_test.append(metrics.f1_score(preds, truth, average=\"macro\"))\n",
    "#         precision_timeline_test.append(metrics.precision_score(preds, truth, average=\"macro\"))\n",
    "#         recall_timeline_test.append(metrics.recall_score(preds,truth, average=\"macro\"))\n",
    "        \n",
    "#         f1_timeline_test_micro.append(metrics.f1_score(preds, truth, average=\"micro\"))\n",
    "#         precision_timeline_test_micro.append(metrics.precision_score(preds, truth, average=\"micro\"))\n",
    "#         recall_timeline_test_micro.append(metrics.recall_score(preds,truth, average=\"micro\"))\n",
    "        \n",
    "#         ff11 = metrics.f1_score(preds, truth, average=None)\n",
    "#         f1_timeline_test_none.append((ff11[1] + ff11[2]) / 2)\n",
    "        \n",
    "#         ppp = metrics.precision_score(preds, truth, average=None)\n",
    "#         precision_timeline_test_none.append((ppp[1] + ppp[2]) / 2)\n",
    "        \n",
    "#         rrr = metrics.recall_score(preds,truth, average=None)\n",
    "#         recall_timeline_test_none.append((rrr[1] + rrr[2]) / 2)\n",
    "        \n",
    "#         accuracy_timeline_test.append(metrics.accuracy_score(preds, truth))\n",
    "        \n",
    "        all_preds_test.append(preds)\n",
    "        # Evaluamos en train para ver si siempre da igual\n",
    "#         loss_tr_eval, preds_train, truth_train, examples_train = evaluate(model, train_dataloader)\n",
    "#         f1_timeline_train.append(filtered_f1(preds_train, truth_train))\n",
    "    tr_loss = 0\n",
    "    global_step = 0\n",
    "    \n",
    "    min_loss = eval_loss_timeline[0]\n",
    "    best_loss_epoch = 0\n",
    "    for epoch, losss in enumerate(eval_loss_timeline):\n",
    "        if losss < min_loss:\n",
    "            best_loss_epoch = epoch\n",
    "            \n",
    "    max_f1 = f1_timeline_dev[0]\n",
    "    best_f1_epoch = 0\n",
    "    for epoch, f11 in enumerate(f1_timeline_dev):\n",
    "        if f11 > max_f1:\n",
    "            best_f1_epoch = epoch\n",
    "    \n",
    "    print(\"BEST EPOCH: {}\".format(best_f1_epoch))\n",
    "\n",
    "#     w.write(\"USANDO LA LOSS:\\n\")\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"DEV\\n\")\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"Macro\")\n",
    "#     w.write(\"{}\\n\".format(precision_timeline_dev[best_loss_epoch]))\n",
    "#     w.write(\"{}\\n\".format(recall_timeline_dev[best_loss_epoch]))\n",
    "#     w.write(\"{}\\n\".format(f1_timeline_dev[best_loss_epoch]))\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"Micro\")\n",
    "#     w.write(\"{}\\n\".format(precision_timeline_dev_micro[best_loss_epoch]))\n",
    "#     w.write(\"{}\\n\".format(recall_timeline_dev_micro[best_loss_epoch]))\n",
    "#     w.write(\"{}\\n\".format(f1_timeline_dev_micro[best_loss_epoch]))\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"Ag vs Fav\")\n",
    "#     w.write(\"{}\\n\".format(precision_timeline_dev_none[best_loss_epoch]))\n",
    "#     w.write(\"{}\\n\".format(recall_timeline_dev_none[best_loss_epoch]))\n",
    "#     w.write(\"{}\\n\".format(f1_timeline_dev_none[best_loss_epoch]))\n",
    "    \n",
    "#     w.write(\"Accu\")\n",
    "#     w.write(\"{}\\n\".format(accuracy_timeline_dev[best_loss_epoch]))\n",
    "    \n",
    "    \n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"TEST\\n\")\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"Macro\")\n",
    "#     w.write(\"{}\\n\".format(precision_timeline_test[best_loss_epoch]))\n",
    "#     w.write(\"{}\\n\".format(recall_timeline_test[best_loss_epoch]))\n",
    "#     w.write(\"{}\\n\".format(f1_timeline_test[best_loss_epoch]))\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"Micro\")\n",
    "#     w.write(\"{}\\n\".format(precision_timeline_test_micro[best_loss_epoch]))\n",
    "#     w.write(\"{}\\n\".format(recall_timeline_test_micro[best_loss_epoch]))\n",
    "#     w.write(\"{}\\n\".format(f1_timeline_test_micro[best_loss_epoch]))\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"Ag vs Fav\")\n",
    "#     w.write(\"{}\\n\".format(precision_timeline_test_none[best_loss_epoch]))\n",
    "#     w.write(\"{}\\n\".format(recall_timeline_test_none[best_loss_epoch]))\n",
    "#     w.write(\"{}\\n\".format(f1_timeline_test_none[best_loss_epoch]))\n",
    "    \n",
    "#     w.write(\"Accu\")\n",
    "#     w.write(\"{}\\n\".format(accuracy_timeline_test[best_loss_epoch]))\n",
    "    \n",
    "    \n",
    "    \n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"USANDO F1:\\n\")\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"DEV\\n\")\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"Macro\")\n",
    "#     w.write(\"{}\\n\".format(precision_timeline_dev[best_f1_epoch]))\n",
    "#     w.write(\"{}\\n\".format(recall_timeline_dev[best_f1_epoch]))\n",
    "#     w.write(\"{}\\n\".format(f1_timeline_dev[best_f1_epoch]))\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"Micro\")\n",
    "#     w.write(\"{}\\n\".format(precision_timeline_dev_micro[best_f1_epoch]))\n",
    "#     w.write(\"{}\\n\".format(recall_timeline_dev_micro[best_f1_epoch]))\n",
    "#     w.write(\"{}\\n\".format(f1_timeline_dev_micro[best_f1_epoch]))\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"Ag vs Fav\")\n",
    "#     w.write(\"{}\\n\".format(precision_timeline_dev_none[best_f1_epoch]))\n",
    "#     w.write(\"{}\\n\".format(recall_timeline_dev_none[best_f1_epoch]))\n",
    "#     w.write(\"{}\\n\".format(f1_timeline_dev_none[best_f1_epoch]))\n",
    "    \n",
    "#     w.write(\"Accu\")\n",
    "#     w.write(\"{}\\n\".format(accuracy_timeline_dev[best_f1_epoch]))\n",
    "    \n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"TEST\\n\")\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"Macro\")\n",
    "#     w.write(\"{}\\n\".format(precision_timeline_test[best_f1_epoch]))\n",
    "#     w.write(\"{}\\n\".format(recall_timeline_test[best_f1_epoch]))\n",
    "#     w.write(\"{}\\n\".format(f1_timeline_test[best_f1_epoch]))\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"Micro\")\n",
    "#     w.write(\"{}\\n\".format(precision_timeline_test_micro[best_f1_epoch]))\n",
    "#     w.write(\"{}\\n\".format(recall_timeline_test_micro[best_f1_epoch]))\n",
    "#     w.write(\"{}\\n\".format(f1_timeline_test_micro[best_f1_epoch]))\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"Ag vs Fav\")\n",
    "#     w.write(\"{}\\n\".format(precision_timeline_test_none[best_f1_epoch]))\n",
    "#     w.write(\"{}\\n\".format(recall_timeline_test_none[best_f1_epoch]))\n",
    "#     w.write(\"{}\\n\".format(f1_timeline_test_none[best_f1_epoch]))\n",
    "    \n",
    "#     w.write(\"Accu\")\n",
    "#     w.write(\"{}\\n\".format(accuracy_timeline_test[best_f1_epoch]))\n",
    "    \n",
    "    # Clasificacion de comunidades\n",
    "#     w.write(\"USANDO LA LOSS\\n\")\n",
    "#     w.write(\"Ag\\n\")\n",
    "#     w.write(community_against_accuracy_timeline[best_loss_epoch])\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"Fav\\n\")\n",
    "#     w.write(community_favor_accuracy_timeline[best_loss_epoch])\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"USANDO F1\\n\")\n",
    "#     w.write(\"Ag\\n\")\n",
    "#     w.write(community_against_accuracy_timeline[best_f1_epoch])\n",
    "#     w.write(\"\\n\")\n",
    "#     w.write(\"Fav\\n\")\n",
    "#     w.write(community_favor_accuracy_timeline[best_f1_epoch])\n",
    "#     w.write(\"\\n\")\n",
    "    \n",
    "#     w.close()\n",
    "    \n",
    "    w_preds.write(\"DEV\\n\")\n",
    "    w_preds.write(str(all_preds_dev[best_f1_epoch]))\n",
    "    w_preds.write(\"\\n\")\n",
    "    w_preds.write(str(truth_dev))\n",
    "    w_preds.write(\"\\n\")\n",
    "    \n",
    "    w_preds.write(\"TEST\\n\")\n",
    "    w_preds.write(str(all_preds_test[best_f1_epoch]))\n",
    "    w_preds.write(\"\\n\")\n",
    "    w_preds.write(str(truth))\n",
    "    w_preds.write(\"\\n\")\n",
    "    \n",
    "#     w_preds.write(\"USANDO LA LOSS\\n\")\n",
    "#     w_preds.write(\"Ag\\n\")\n",
    "#     w_preds.write(\"{}\\n\".format(community_against_preds_timeline[best_loss_epoch]))\n",
    "#     w_preds.write(\"\\n\")\n",
    "#     w_preds.write(\"Fav\\n\")\n",
    "#     w_preds.write(\"{}\\n\".format(community_favor_preds_timeline[best_loss_epoch]))\n",
    "#     w_preds.write(\"\\n\")\n",
    "    \n",
    "#     w_preds.write(\"USANDO F1\\n\")\n",
    "#     w_preds.write(\"Ag\\n\")\n",
    "#     w_preds.write(\"{}\\n\".format(community_against_preds_timeline[best_f1_epoch]))\n",
    "#     w_preds.write(\"\\n\")\n",
    "#     w_preds.write(\"Fav\\n\")\n",
    "#     w_preds.write(\"{}\\n\".format(community_favor_preds_timeline[best_f1_epoch]))\n",
    "#     w_preds.write(\"\\n\")\n",
    "    w_preds.close()\n",
    "    \n",
    "    return all_preds_dev, truth_dev, all_preds_test, truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0128 12:10:47.401937 140620505528064 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /home/dfurman/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "I0128 12:10:54.622985 140620505528064 modeling_utils.py:480] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0128 12:10:54.623744 140620505528064 modeling_utils.py:483] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:16<00:00, 25.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0128 12:12:12.195535 140620505528064 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /home/dfurman/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "I0128 12:12:18.836457 140620505528064 modeling_utils.py:480] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0128 12:12:18.837213 140620505528064 modeling_utils.py:483] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]/home/dfurman/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:43<00:00, 14.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0128 12:13:03.578103 140620505528064 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /home/dfurman/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "I0128 12:13:10.342358 140620505528064 modeling_utils.py:480] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0128 12:13:10.343117 140620505528064 modeling_utils.py:483] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:18<00:00, 26.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0128 12:14:29.690497 140620505528064 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /home/dfurman/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "I0128 12:14:36.431648 140620505528064 modeling_utils.py:480] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0128 12:14:36.432733 140620505528064 modeling_utils.py:483] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:43<00:00, 14.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0128 12:15:21.476318 140620505528064 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /home/dfurman/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "I0128 12:15:28.287329 140620505528064 modeling_utils.py:480] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0128 12:15:28.288210 140620505528064 modeling_utils.py:483] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:18<00:00, 26.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0128 12:16:48.014286 140620505528064 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /home/dfurman/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "I0128 12:16:54.804874 140620505528064 modeling_utils.py:480] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0128 12:16:54.805644 140620505528064 modeling_utils.py:483] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:44<00:00, 14.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0128 12:17:39.923736 140620505528064 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /home/dfurman/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "I0128 12:17:46.690092 140620505528064 modeling_utils.py:480] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0128 12:17:46.691021 140620505528064 modeling_utils.py:483] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:18<00:00, 26.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0128 12:19:06.585733 140620505528064 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /home/dfurman/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "I0128 12:19:13.290684 140620505528064 modeling_utils.py:480] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0128 12:19:13.291602 140620505528064 modeling_utils.py:483] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:43<00:00, 14.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0128 12:19:58.552865 140620505528064 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /home/dfurman/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "I0128 12:20:05.308606 140620505528064 modeling_utils.py:480] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0128 12:20:05.309367 140620505528064 modeling_utils.py:483] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:18<00:00, 26.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0128 12:21:25.183453 140620505528064 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /home/dfurman/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "I0128 12:21:31.961474 140620505528064 modeling_utils.py:480] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0128 12:21:31.962461 140620505528064 modeling_utils.py:483] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:44<00:00, 14.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0128 12:22:17.243832 140620505528064 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /home/dfurman/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "I0128 12:22:24.137727 140620505528064 modeling_utils.py:480] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0128 12:22:24.138560 140620505528064 modeling_utils.py:483] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:18<00:00, 26.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0128 12:23:44.049049 140620505528064 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /home/dfurman/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "I0128 12:23:50.530630 140620505528064 modeling_utils.py:480] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0128 12:23:50.531613 140620505528064 modeling_utils.py:483] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:43<00:00, 14.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(n_gpu):\n",
    "    for lr in [1e-04, 5e-06]:\n",
    "        for sched in [\"constant\", \"linear\"]:\n",
    "            for batch_size in [4, 16]:\n",
    "                model = model_class.from_pretrained(bert_type, from_tf=False, config=config)\n",
    "                model.to(device)\n",
    "                all_preds_dev, truth_dev, all_preds_test, truth = train(train_dataset, model, tokenizer, labels, pad_token_label_id, dev_dataloader, test_dataloader, scheduler_type=sched, lr=lr, train_batch_size=batch_size, num_train_epochs=3)\n",
    "#                 if os.path.exists(\"pretrained_models\"):\n",
    "#                     if not os.path.exists(\"pretrained_models/bert-large-uncased-8epochs-twitter-stance-{}_{}_{}\".format(sched, str(lr), str(batch_size))):\n",
    "#                         os.mkdir(\"pretrained_models/bert-large-uncased-8epochs-twitter-stance-{}_{}_{}\".format(sched, str(lr), str(batch_size)))\n",
    "#                     model.save_pretrained(\"pretrained_models/bert-large-uncased-8epochs-twitter-stance-{}_{}_{}\".format(sched, str(lr), str(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = open(\"twitter-stance_large_uncased_atheism_bert-large-uncased_linear_1e-06_16_preds\", \"r\")\n",
    "\n",
    "for idx, line in enumerate(r):\n",
    "    if idx == 1:\n",
    "        preds_eval = line[1:-2].split(\" \")\n",
    "    if idx == 2:\n",
    "        truth_eval = line[1:-2].split(\" \")\n",
    "    if idx == 4:\n",
    "        preds_test = line[1:-2].split(\" \")\n",
    "    if idx == 5:\n",
    "        truth_test = line[1:-2].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6183594272715108"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(truth_eval, preds_eval, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
